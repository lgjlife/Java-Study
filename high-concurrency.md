<h1>高并发</h1>


<span id="menu"></span>

<!-- TOC -->

- [1. 高并发网站设计](#1-高并发网站设计)
  - [1.1. 概述](#11-概述)
    - [1.1.1. 高并发原则](#111-高并发原则)
    - [1.1.2. 高可用原则](#112-高可用原则)
  - [1.2. 负载均衡](#12-负载均衡)
    - [1.2.1. 什么是负载均衡](#121-什么是负载均衡)
    - [1.2.2. 硬件负载均衡](#122-硬件负载均衡)
    - [1.2.3. 四层和七层负载均衡的区别？](#123-四层和七层负载均衡的区别)
      - [1.2.3.1. 简介](#1231-简介)
      - [1.2.3.2. 技术原理上的区别](#1232-技术原理上的区别)
      - [1.2.3.3. 应用场景的需求](#1233-应用场景的需求)
      - [1.2.3.4. 七层应用需要考虑的问题。](#1234-七层应用需要考虑的问题)
      - [1.2.3.5. Nginx、LVS及HAProxy负载均衡软件的优缺点](#1235-nginxlvs及haproxy负载均衡软件的优缺点)
    - [1.2.4. 负载均衡的算法](#124-负载均衡的算法)
      - [1.2.4.1. 随机算法](#1241-随机算法)
      - [1.2.4.2. 轮询及加权轮询](#1242-轮询及加权轮询)
      - [1.2.4.3. 最小连接及加权最小连接](#1243-最小连接及加权最小连接)
      - [1.2.4.4. 哈希算法](#1244-哈希算法)
      - [1.2.4.5. IP地址散列](#1245-ip地址散列)
      - [1.2.4.6. URL散列](#1246-url散列)
      - [1.2.4.7. 按负载选择](#1247-按负载选择)
      - [1.2.4.8. 一致性哈希算法](#1248-一致性哈希算法)
    - [1.2.5. 负载均衡的实现（DNS > 数据链路层 > IP层 > Http层）](#125-负载均衡的实现dns--数据链路层--ip层--http层)
      - [1.2.5.1. DNS域名解析负载均衡（延迟）](#1251-dns域名解析负载均衡延迟)
      - [1.2.5.2. 数据链路层负载均衡(LVS)](#1252-数据链路层负载均衡lvs)
      - [1.2.5.3. IP负载均衡(SNAT)](#1253-ip负载均衡snat)
      - [1.2.5.4. HTTP重定向负载均衡(少见)](#1254-http重定向负载均衡少见)
      - [1.2.5.5. 反向代理负载均衡(nginx)](#1255-反向代理负载均衡nginx)
  - [1.3. 隔离](#13-隔离)
    - [1.3.1. 概述](#131-概述)
  - [1.4. 限流](#14-限流)
    - [1.4.1. 概述](#141-概述)
    - [1.4.2. 限流算法](#142-限流算法)
      - [1.4.2.1. 计数器法](#1421-计数器法)
      - [1.4.2.2. 滑动窗口](#1422-滑动窗口)
      - [1.4.2.3. 漏桶算法](#1423-漏桶算法)
      - [1.4.2.4. 令牌桶算法](#1424-令牌桶算法)
    - [1.4.3. 应用级���流](#143-应用级流)
      - [1.4.3.1. 限流总并发数/连接/请求数](#1431-限流总并发数连接请求数)
      - [1.4.3.2. 限流总资源数](#1432-限流总资源数)
      - [1.4.3.3. 限流某个接口的总并发数/请求数](#1433-限流某个接口的总并发数请求数)
      - [1.4.3.4. 限流某个接口的时间窗请求数](#1434-限流某个接口的时间窗请求数)
      - [1.4.3.5. 平滑限流某个接口的请求数](#1435-平滑限流某个接口的请求数)
    - [1.4.4. 分布式限流](#144-分布式限流)
      - [1.4.4.1. Redis与Lua](#1441-redis与lua)
      - [1.4.4.2. Nginx](#1442-nginx)
  - [1.5. 降级](#15-降级)
    - [1.5.1. 降级概念](#151-降级概念)
    - [1.5.2. 使用Hystrix实现降级](#152-使用hystrix实现降级)
      - [1.5.2.1. 降级Demo](#1521-降级demo)
      - [1.5.2.2. 降级参数](#1522-降级参数)
      - [1.5.2.3. 熔断](#1523-熔断)
      - [1.5.2.4. 采样统计](#1524-采样统计)
      - [1.5.2.5. 线程/信号量隔离](#1525-线程信号量隔离)
  - [1.6. 回滚机制](#16-回滚机制)
    - [1.6.1. 事务回滚](#161-事务回滚)
    - [1.6.2. 代码库回滚](#162-代码库回滚)
    - [1.6.3. 部署版本回滚](#163-部署版本回滚)
    - [1.6.4. 静态资源回滚](#164-静态资源回滚)
  - [1.7. 压测与预案](#17-压测与预案)
    - [1.7.1. 系统压测](#171-系统压测)
      - [1.7.1.1. 线下压测](#1711-线下压测)
      - [1.7.1.2. 线上压测](#1712-线上压测)
    - [1.7.2. 系统优化和容灾](#172-系统优化和容灾)
  - [1.8. 缓存](#18-缓存)
    - [1.8.1. 应用级缓存](#181-应用级缓存)
      - [1.8.1.1. 缓存命中率](#1811-缓存命中率)
      - [1.8.1.2. 缓存回收策略](#1812-缓存回收策略)
      - [1.8.1.3. 回收算法](#1813-回收算法)
        - [1.8.1.3.1. FIFO](#18131-fifo)
        - [1.8.1.3.2. LRU](#18132-lru)
        - [1.8.1.3.3. LFU](#18133-lfu)
        - [1.8.1.3.4. LRFU](#18134-lrfu)
      - [1.8.1.4. Java 缓存类型](#1814-java-缓存类型)
        - [1.8.1.4.1. 堆缓存](#18141-堆缓存)
        - [1.8.1.4.2. 堆外缓存](#18142-堆外缓存)
        - [1.8.1.4.3. 磁盘缓存](#18143-磁盘缓存)
        - [1.8.1.4.4. 分布式缓存](#18144-分布式缓存)
        - [1.8.1.4.5. 多级缓存](#18145-多级缓存)
      - [1.8.1.5. 应用级缓存示例](#1815-应用级缓存示例)
      - [1.8.1.6. 缓存使用模式实践](#1816-缓存使用模式实践)
        - [1.8.1.6.1. Cache-Aside](#18161-cache-aside)
        - [1.8.1.6.2. Cache-As-SOR](#18162-cache-as-sor)
        - [1.8.1.6.3. Read-Through](#18163-read-through)
        - [1.8.1.6.4. Write-Through](#18164-write-through)
        - [1.8.1.6.5. Write-Behind](#18165-write-behind)
        - [1.8.1.6.6. Copy-Pattern](#18166-copy-pattern)
      - [1.8.1.7. 缓存一致性处理](#1817-缓存一致性处理)
      - [1.8.1.8. 缓存异常处理](#1818-缓存异常处理)
        - [1.8.1.8.1. 缓存穿透](#18181-缓存穿透)
        - [1.8.1.8.2. 缓存击穿](#18182-缓存击穿)
        - [1.8.1.8.3. 缓存雪崩](#18183-缓存雪崩)
        - [1.8.1.8.4. 解决方案](#18184-解决方案)
        - [1.8.1.8.5. 缓存并发问题](#18185-缓存并发问题)
    - [1.8.2. WEB缓存&客户端缓存](#182-web缓存客户端缓存)
      - [1.8.2.1. 应用APP缓存](#1821-应用app缓存)
      - [1.8.2.2. 浏览器缓存](#1822-浏览器缓存)
      - [1.8.2.3. CDN缓存](#1823-cdn缓存)
      - [1.8.2.4. NGINX缓存](#1824-nginx缓存)
    - [1.8.3. 多级缓存](#183-多级缓存)
      - [1.8.3.1. 多级缓存介绍](#1831-多级缓存介绍)
      - [1.8.3.2. 如何缓存数据](#1832-如何缓存数据)
        - [1.8.3.2.1. 过期与不过期](#18321-过期与不过期)
        - [1.8.3.2.2. 维度化缓存与增量缓存](#18322-维度化缓存与增量缓存)
        - [1.8.3.2.3. 大Value 缓存](#18323-大value-缓存)
        - [1.8.3.2.4. 热点缓存](#18324-热点缓存)
      - [1.8.3.3. 更新缓存和原子性](#1833-更新缓存和原子性)
    - [1.8.4. Feed](#184-feed)
      - [1.8.4.1. 概念](#1841-概念)
      - [1.8.4.2. 特征](#1842-特征)
      - [1.8.4.3. 分类](#1843-分类)
      - [1.8.4.4. 实现](#1844-实现)
      - [1.8.4.5. TableStore](#1845-tablestore)
      - [1.8.4.6. 存储系统选择](#1846-存储系统选择)
      - [1.8.4.7. 存储Feed消息](#1847-存储feed消息)
      - [1.8.4.8. 推送方案](#1848-推送方案)
      - [1.8.4.9. 推送系统](#1849-推送系统)
      - [1.8.4.10. 使用TableStore的架构图](#18410-使用tablestore的架构图)
      - [1.8.4.11. 实践](#18411-实践)
      - [1.8.4.12. 最后](#18412-最后)
  - [1.9. 系统稳定性](#19-系统稳定性)
    - [1.9.1. 在线日志分析](#191-在线日志分析)
      - [1.9.1.1. 日志分析常用命令](#1911-日志分析常用命令)
    - [1.9.2. 集群监控](#192-集群监控)
      - [1.9.2.1. 监控指标](#1921-监控指标)
    - [1.9.3. 流量控制](#193-流量控制)
    - [1.9.4. 性能优化](#194-性能优化)
    - [1.9.5. Java故障排查](#195-java故障排查)

<!-- /TOC -->

# 1. 高并发网站设计

## 1.1. 概述
<a href="#menu" >目录</a>

### 1.1.1. 高并发原则
* 无状态
    * 应用无状态，可以方便的进行集群扩展
    * 应用的配置从配置文件中读取，或者从配置中心读取
* 拆分
    * 服务垂直拆分，合理利用计算机资源
    * 降低某个模块出现故障导致其他模块无法使用的问题
    * 拆分原则
        * 系统维度，按照业务进行拆分，比如用户服务，积分服务
        * 功能维度，系统维度拆分之后再进行进一步按照功能进行拆分,比如积分分为领取系统，消费积分系统
        * 读写维度，按照读写比例进行拆分、读写分离
        * 模块维度，按照基础或者代码维护特征进行拆分，比如MVC架构
* 服务化
    * 系统拆分之后的微服务化
    * 进程内服务->单机远程服务->集群手动注册服务->自动注册和发现服务->服务的分组／隔离／路由->服务治理如限流／黑白名单
* 消息队列
    * 服务解耦
    * 异步处理
    * 流量消峰
* 数据异构
    * 分库分表
* 缓存
    * 客户端缓存
        * 浏览器缓存
        * 客户端应用缓存
    * 代理缓存
    * 广域网缓存
        * CDN
        * 镜像服务器
        * P2P技术
    * 进程缓存
    * 分布式缓存
* 并发化
    * 多线程处理
### 1.1.2. 高可用原则
<a href="#menu" >目录</a>

* 降级
    * 降级为在高并发下，将某些应用或者**某些功能暂停使用**，减少对资源的争抢，保障系统可用
    * 降级处理
        * 开关集中化，可以通过服务配置中心进行降级操作
        * 可降级的多级读服务，比如降级为只读本地缓存，只读分布式缓存
        * 开关前置化，比如并发流量大时，在Nginx处进行限流
        * 业务降级
            * 不重要的业务暂停工作
            * 同步调用改异步调用，保证数据最终一致即可
* 限流
    * 防止恶意请求流量，恶意攻击，防止流量超出系统峰值。
    * 恶意请求流量只访问到cache
    * 对于穿透到后端的可以考虑Nginx的Limit模块处理
    * 对于恶意IP可以使用nginx deny进行屏蔽
    
* 切流量
    * 机房挂了或者某台服务器挂了需要切流量
    * DNS:切换机房入口
    * HttpsDNS,在客户端分配好流量入口，绕过运营商的LocalDNS,并实现更精准流量调度
    * LVS/HAProxy:切换故障的Nginx接入层
    * Nginx:切换故障的应用层
* 可回滚
    * 版本回滚，新版本上线出现问题，可以回滚到之前的版本
* 集群部署，负载均衡，避免单点故障
    * 硬件负载均衡
    * 软件负载均衡
* 设计可容错的系统
    * 当某个服务不可用时，请求该服务应当有容错处理，避免频繁地重试。或者阻塞等待。造成系统线程武无限增长，最后宕积
* 限制使用资源
    * 比如使用堆内存时，应当限制最大内存限值，避免无限制的使用造成频繁地GC
    * 线程以及线程池中的无限队列不合适使用都有可能造成内存溢出
    * 循环使用也有可能出现CPU飙升
    * 限制网络的使用， 频繁地建立连接和关闭连接非常地耗性能,可以使用长连接或者连接池
* 热备
* 使用多机房'


    
## 1.2. 负载均衡
<a href="#menu" >目录</a>

### 1.2.1. 什么是负载均衡
　互联网早期，业务流量比较小并且业务逻辑比较简单，单台服务器便可以满足基本的需求；但随着互联网的发展，业务流量越来越大并且业务逻辑也越来越复杂，单台机器的性能问题以及单点问题凸显了出来，因此需要多台机器来进行性能的水平扩展以及避免单点故障。但是要如何将不同的用户的流量分发到不同的服务器上面呢？

　 早期的方法是使用DNS做负载，通过给客户端解析不同的IP地址，让客户端的流量直接到达各个服务器。但是这种方法有一个很大的缺点就是延时性问题，在做出调度策略改变以后，由于DNS各级节点的缓存并不会及时的在客户端生效，而且DNS负载的调度策略比较简单，无法满足业务需求，因此就出现了负载均衡。


　客户端的流量首先会到达负载均衡服务器，由负载均衡服务器通过一定的调度算法将流量分发到不同的应用服务器上面，同时负载均衡服务器也会对应用服务器做周期性的健康检查，当发现故障节点时便动态的将节点从应用服务器集群中剔除，以此来保证应用的高可用。


　负载均衡又分为四层负载均衡和七层负载均衡。四层负载均衡工作在OSI模型的传输层，主要工作是转发，它在接收到客户端的流量以后通过修改数据包的地址信息将流量转发到应用服务器。

　七层负载均衡工作在OSI模型的应用层，因为它需要解析应用层流量，所以七层负载均衡在接到客户端的流量以后，还需要一个完整的TCP/IP协议栈。七层负载均衡会与客户端建立一条完整的连接并将应用层的请求流量解析出来，再按照调度算法选择一个应用服务器，并与应用服务器建立另外一条连接将请求发送过去，因此七层负载均衡的主要工作就是代理。

### 1.2.2. 硬件负载均衡

硬件负载均衡解决方案是直接在服务器和外部网络间安装负载均衡设备，这种设备我们通常称之为负载均衡器，由于专门的设备完成网络请求转发的任务，独立于操作系统，整体性能高，负载均衡策略多样化，流量管理智能化。

**硬件负载均衡的优缺点是什么？**

* 优点
    * 直接连接交换机,处理网络请求能力强，与系统无关，负载性可以强。可以应用于大量设施、适应大访问量、使用简单。
* 缺点
    * 成本高，配置冗余．即使网络请求分发到服务器集群，负载均衡设施却是单点配置；无法有效掌握服务器及应使用状态.

**使用的注意事项以及应用的场景？**

注意事项，需要注意的是硬件负载均衡技术只专注网络判断，不考虑业务系统与应用使用的情况。有时候系统处理能力已经达到了瓶颈，但是此时网络并没有异常，由于硬件负载均衡并没有察觉到应用服务器的异常，还是让流量继续进入到应用服务器。

**硬件负载均衡器实现哪些功能？**

目前市面上有NetScaler, F5, Radware, Array 等产品，基本实现原理大致相同，我们这里把使用的比较多的 F5做为例子给大家做简单解释，算是窥豹一斑。

**多链路负载均衡**

关键业务都需要安排和配置多条ISP（网络服务供应商）接入链路以保证网络服务的质量。如果某个ISP停止服务或者服务异常了，那么可以利用另一个ISP替代服务，提高了网络的可用性。不同的ISP有不同自治域,因此需要考虑两种情况:INBOUND 和 OUTBOUND。

INBOUND，来自网络的请求信息。F5 分别绑定两个ISP 服务商的公网地址,解析来自两个ISP服务商的DNS解析请求。F5可以根据服务器状况和响应情况对DNS进行发送,也可以通过多条链路分别建立DNS连接。
OUTBOUND，返回给请求者的应答信息。F5可以将流量分配到不同的网络接口，并做源地址的NAT（网络地址转换）,即通过IP地址转换为源请求地址。也可以用接口地址自动映射,保证数据包返回时能够被源头正确接收。

**防火墙负载均衡**

针对大量网络请求的情况单一防火墙的能力就有限了，而且防火墙本身要求数据同进同出，为了解决多防火墙负载均衡的问题，F5提出了防火墙负载均衡的“防火墙三明治"方案

防火墙会对用户会话的双向数据流进行监控，从而确定数据的合法性。如果采取多台防火墙进行负载均衡，有可能会造成同一个用户会话的双向数据在多台防火墙上都进行处理，而单个防火墙上看不到完成用户会话的信息，就会认为数据非法因此抛弃数据。所以在每个防火墙的两端要架设四层交换机，可以在作流量分发的同时，维持用户会话的完整性，使同一用户的会话由一个防火墙来处理。而F5 会协调上述方案的配置和实现，把“交换机”，“防火墙”，“交换机”夹在了一起好像三明治一样。

![](http://5b0988e595225.cdn.sohucs.com/images/20190123/daf62f34338741818adada510f393b91.jpeg)

防火墙“三明治”

**服务器负载均衡**

对于应用服务器服务器可以在F5上配置并且实现负载均衡，F5可以检查服务器的健康状态如果发现故障，将其从负载均衡组中移除。
F5 对于外网而言有一个真实的IP，对于内网的每个服务器都生成一个虚拟IP，进行负载均衡和管理工作。因此,它能够为大量的基于TCP/IP的网络应用提供服务器负载均衡服务。
根据服务类型不同定义不同的服务器群组。
根据不同服务端口将流量导向对应的服务器。甚至可以对VIP用户的请求进行特殊的处理，把这类请求导入到高性能的服务器使VIP客户得到最好的服务响应。
根据用户访问内容的不同将流量导向指定服务器。

* 可用性
    * 自身高可用性，在双机冗余模式下工作时实现毫秒级切换。
    * 设备冗余电源模块可选。
    * 每台设备通过心跳线监控其他设备的电频，发现故障的时候可以完成自动切换。
    * 链路冗余：对链路故障进行实时检测，一旦发现故障进行自动流量切换，过程透明。
    * 服务器冗余：对服务器进行心跳检测，一旦发现故障立即从服务器列表中移除，如果恢复工作又重新加入到服务器列表中。

* 安全性
    * 站点安全防护
    * 拆除空闲连接防止拒绝服务攻击
    * 能够执行源路由跟踪防止IP欺骗
    * 拒绝没有ACK缓冲确认的SYN防止SYN攻击
    * 拒绝teartop和land攻击;保护自己和服务器免受ICMP攻击
* 系统管理
    * 提供浏览器级别管理软件，Web图形用户界面。
    * 总结：对于高并发，高访问量的互联网应用可以考虑加入硬件负载均衡器作为接入层，协助代理层的软件负载均衡器进行负载均衡的工作。硬件负载均衡器的特点是独立于操作系统，处理大访问量，费用高。从功能上来说支持多链路，多服务器，多防火墙的负载均衡，在可用性和安全性上也有良好的表现

### 1.2.3. 四层和七层负载均衡的区别？
<a href="#menu" >目录</a>

#### 1.2.3.1. 简介

* 所谓四层就是基于IP+端口的负载均衡；七层就是基于URL等应用层信息的负载均衡；**同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。 换句换说，二层负载均衡会通过一个虚拟MAC地址接收请求，然后再分配到真实的MAC地址；三层负载均衡会通过一个虚拟IP地址接收请求，然后再分配到真实的IP地址；四层通过虚拟IP+端口接收请求，然后再分配到真实的服务器；七层通过虚拟的URL或主机名接收请求，然后再分配到真实的服务器。

* 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。** 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。

负载均衡器通常称为四层交换机或七层交换机。四层交换机主要分析IP层及TCP/UDP层，实现四层流量负载均衡。七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息。

负载均衡分为L4 switch（四层交换），即在OSI第4层工作，就是TCP层啦。此种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）。例子：LVS，F5。

另一种叫做L7 switch（七层交换），OSI的最高层，应用层。此时，该Load Balancer能理解应用协议。例子： haproxy，MySQL Proxy。

注意：上面的很多Load Balancer既可以做四层交换，也可以做七层交换

#### 1.2.3.2. 技术原理上的区别
　所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。

　以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。

　所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。

　以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
　
　负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。那么，为什么还需要七层负载均衡呢？

#### 1.2.3.3. 应用场景的需求
　七层应用负载的好处，是使得整个网络更"智能化", 参考我们之前的另外一篇专门针对HTTP应用的优化的介绍，就可以基本上了解这种方式的优势所在。例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。

　当然这只是七层应用的一个小案例，从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性。很多在后台，(例如Nginx或者Apache)上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。

　另外一个常常被提到功能就是安全性。网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击，通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service(DoS)的目的。

　从技术原理上也可以看出，四层模式下这些SYN攻击都会被转发到后端的服务器上；而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营。另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。

　现在的7层负载均衡，主要还是着重于应用广泛的HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。

#### 1.2.3.4. 七层应用需要考虑的问题。
是否真的必要，七层应用的确可以提高流量智能化，同时必不可免的带来设备配置复杂，负载均衡压力增高以及故障排查上的复杂性等问题。在设计系统时需要考虑四层七层同时应用的混杂情况。

是否真的可以提高安全性。例如SYN Flood攻击，七层模式的确将这些流量从服务器屏蔽，但负载均衡设备本身要有强大的抗DDoS能力，否则即使服务器正常而作为中枢调度的负载均衡设备故障也会导致整个应用的崩溃。

是否有足够的灵活度。七层应用的优势是可以让整个应用的流量智能化，但是负载均衡设备需要提供完善的七层功能，满足客户根据不同情况的基于应用的调度。最简单的一个考核就是能否取代后台Nginx或者Apache等服务器上的调度功能。能够提供一个七层应用开发接口的负载均衡设备，可以让客户根据需求任意设定功能，才真正有可能提供强大的灵活性和智能性。

#### 1.2.3.5. Nginx、LVS及HAProxy负载均衡软件的优缺点

负载均衡 （Load Balancing） 建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力，同时能够提高网络的灵活性和可用性。

Nginx/LVS/HAProxy是目前使用最广泛的三种负载均衡软件。

一般对负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术。具体的应用需求还得具体分析，如果是中小型的Web应用，比如日PV小于1000万，用Nginx就完全可以了；如果机器不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或重要的服务，且服务器比较多时，可以考虑用LVS。

一种是通过硬件来进行，常见的硬件有比较昂贵的F5和Array等商用的负载均衡器，它的优点就是有专业的维护团队来对这些服务进行维护、缺点就是花销太大，所以对于规模较小的网络服务来说暂时还没有需要使用；另外一种就是类似于Nginx/LVS/HAProxy的基于 Linux的开源免费的负载均衡软件，这些都是通过软件级别来实现，所以费用非常低廉。

目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx/HAProxy+ Keepalived作负载均衡器；后端采用 MySQL数据库一主多从和读写分离，采用LVS+Keepalived的架构。当然要根据项目具体需求制定方案。

**Nginx的优点是：**

* 工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。
* Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；相反LVS对网络稳定性依赖比较大。
* Nginx安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来。LVS的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。
* 可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。
* Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。
* Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。
* Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可以考虑用其作为反向代理加速器。
* Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有 lighttpd了，不过 lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃。
* Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多。

**Nginx的缺点是：**

* Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点。
* 对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测。不支持Session的直接保持，但能通过ip_hash来解决。


LVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器，它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)。

**LVS的优点是：**

* 抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。
* 配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。
* 工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived。
* 无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会受到大流量的影响。
* 应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等。

**LVS的缺点是：**

* 软件本身不支持正则表达式处理，不能做动静分离；而现在许多网站在这方面都有较强的需求，这个是Nginx/HAProxy+Keepalived的优势所在。
* 如果是网站应用比较庞大的话，LVS/DR+Keepalived实施起来就比较复杂了，特别后面有 Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了，相对而言，Nginx/HAProxy+Keepalived就简单多了。

**HAProxy的特点是：**

* HAProxy也是支持虚拟主机的。
* HAProxy的优点能够补充Nginx的一些缺点，比如支持Session的保持，Cookie的引导；同时支持通过获取指定的url来检测后端服务器的状态。
* HAProxy跟LVS类似，本身就只是一款负载均衡软件；单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的。
* HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，大家可以用LVS+Keepalived对MySQL主从做负载均衡。
* HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：
    * roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；
    * static-rr，表示根据权重，建议关注；
    * leastconn，表示最少连接者先处理，建议关注；
    * source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；
    * ri，表示根据请求的URI；
    * rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；
    * hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；
    * rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。

**Nginx和LVS对比的总结：**

* Nginx工作在网络的7层，所以它可以针对http应用本身来做分流策略，比如针对域名、目录结构等，相比之下LVS并不具备这样的功能，所以Nginx单凭这点可利用的场合就远多于LVS了；但Nginx有用的这些功能使其可调整度要高于LVS，所以经常要去触碰触碰，触碰多了，人为出问题的几率也就会大。
* Nginx对网络稳定性的依赖较小，理论上只要ping得通，网页访问正常，Nginx就能连得通，这是Nginx的一大优势！Nginx同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；LVS就比较依赖于网络环境，目前来看服务器在同一网段内并且LVS使用direct方式分流，效果较能得到保证。另外注意，LVS需要向托管商至少申请多一个ip来做Visual IP，貌似是不能用本身的IP来做VIP的。要做好LVS管理员，确实得跟进学习很多有关网络通信方面的知识，就不再是一个HTTP那么简单了。
* Nginx安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。LVS的安装和配置、测试就要花比较长的时间了；LVS对网络依赖比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦得多。
* Nginx也同样能承受很高负载且稳定，但负载度和稳定度差LVS还有几个等级：Nginx处理所有流量所以受限于机器IO和配置；本身的bug也还是难以避免的。
* Nginx可以检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前LVS中 ldirectd也能支持针对服务器内部的情况来监控，但LVS的原理使其不能重发请求。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而恼火。
* Nginx对请求的异步处理可以帮助节点服务器减轻负载，假如使用 apache直接对外服务，那么出现很多的窄带链接时apache服务器将会占用大 量内存而不能释放，使用多一个Nginx做apache代理的话，这些窄带链接会被Nginx挡住，apache上就不会堆积过多的请求，这样就减少了相当多的资源占用。这点使用squid也有相同的作用，即使squid本身配置为不缓存，对apache还是有很大帮助的。
* Nginx能支持http、https和email（email的功能比较少用），LVS所支持的应用在这点上会比Nginx更多。在使用上，一般最前端所采取的策略应是LVS，也就是DNS的指向应为LVS均衡器，LVS的优点令它非常适合做这个任务。重要的ip地址，最好交由LVS托管，比如数据库的 ip、webservice服务器的ip等等，这些ip地址随着时间推移，使用面会越来越大，如果更换ip则故障会接踵而至。所以将这些重要ip交给 LVS托管是最为稳妥的，这样做的唯一缺点是需要的VIP数量会比较多。Nginx可作为LVS节点机器使用，一是可以利用Nginx的功能，二是可以利用Nginx的性能。当然这一层面也可以直接使用squid，squid的功能方面就比Nginx弱不少了，性能上也有所逊色于Nginx。Nginx也可作为中层代理使用，这一层面Nginx基本上无对手，唯一可以撼动Nginx的就只有lighttpd了，不过lighttpd目前还没有能做到 Nginx完全的功能，配置也不那么清晰易读。另外，中层代理的IP也是重要的，所以中层代理也拥有一个VIP和LVS是最完美的方案了。具体的应用还得具体分析，如果是比较小的网站（日PV小于1000万），用Nginx就完全可以了，如果机器也不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或者重要的服务，机器不发愁的时候，要多多考虑利用LVS。

现在对网络负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术：
* 第一阶段：利用Nginx或HAProxy进行单点的负载均衡，这一阶段服务器规模刚脱离开单服务器、单数据库的模式，需要一定的负载均衡，但是仍然规模较小没有专业的维护团队来进行维护，也没有需要进行大规模的网站部署。这样利用Nginx或HAproxy就是第一选择，此时这些东西上手快， 配置容易，在七层之上利用HTTP协议就可以。这时是第一选择。
* 第二阶段：随着网络服务进一步扩大，这时单点的Nginx已经不能满足，这时使用LVS或者商用Array就是首要选择，Nginx此时就作为LVS或者Array的节点来使用，具体LVS或Array的是选择是根据公司规模和预算来选择，Array的应用交付功能非常强大，本人在某项目中使用过，性价比也远高于F5，商用首选，但是一般来说这阶段相关人才跟不上业务的提升，所以购买商业负载均衡已经成为了必经之路。
* 第三阶段：这时网络服务已经成为主流产品，此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升，这时无论从开发适合自身产品的定制，以及降低成本来讲开源的LVS，已经成为首选，这时LVS会成为主流。

最终形成比较理想的基本架构为：Array/LVS — Nginx/Haproxy — Squid/Varnish — AppServer。



### 1.2.4. 负载均衡的算法
<a href="#menu" >目录</a>

#### 1.2.4.1. 随机算法
* Random随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。
* 加权随机
* 如果业务处理机器在处理各种请求时所需要的资源相差不多，那么采用随机法能保持后端的机器的负载相对是均衡的

#### 1.2.4.2. 轮询及加权轮询
* 轮询(Round Robbin)当服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法。 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。
* 加权轮询(Weighted Round Robbin)为轮询中的每台服务器附加一定权重的算法。比如服务器1权重1，服务器2权重2，服务器3权重3，则顺序为1-2-2-3-3-3-1-2-2-3-3-3- ......
* 如果业务处理机器在处理各种请求时所需要的资源相差不多，那么采用随机法能保持后端的机器的负载相对是均衡的

#### 1.2.4.3. 最小连接及加权最小连接
* 最少连接(Least Connections)在多个服务器中，与处理连接数(会话数)最少的服务器进行通信的算法。即使在每台服务器处理能力各不相同，每笔业务处理量也不相同的情况下，也能够在一定程度上降低服务器的负载。
加权最少连接(Weighted Least Connection)为最少连接算法中的每台服务器附加权重的算法，该算法事先为每台服务器分配处理连接的数量，并将客户端请求转至连接数最少的服务器上。

需要注意，当集群中一台机器重启时，此时这台机器的连接最少，最新的请求就会发送到该机器，很可能撑爆该机器．

#### 1.2.4.4. 哈希算法

* 对请求信息进行hash
* 普通哈希
* 一致性哈希，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。
* 由于需要进行hash计算，因此有一定的性能损耗

#### 1.2.4.5. IP地址散列
* 通过管理发送方IP和目的地IP地址的散列，将来自同一发送方的分组(或发送至同一目的地的分组)统一转发到相同服务器的算法。当客户端有一系列业务需要处理而必须和一个服务器反复通信时，该算法能够以流(会话)为单位，保证来自相同客户端的通信能够一直在同一服务器中进行处理。

#### 1.2.4.6. URL散列
* 通过管理客户端请求URL信息的散列，将发送至相同URL的请求转发至同一服务器的算法。

#### 1.2.4.7. 按负载选择
根据实际业务处理机器的负载进行选择，选择负载相对比较低的来进行处理，尽可能保证所有机器负载均衡．

此方法用于业务处理机器在处理多种请求时所需资源相差较大的情况，可以避免消耗资源较多的请求都发送到同一个机器上，出现请求均衡但是负载不均衡的现象．

这种方法要求负载均衡机器每隔一段时间就向实际业务处理机器搜集其负载的情况，这种方式会给负载均衡机器增加一些负担，并且一旦搜集负载状况过程中出现较大的延迟或搜集不到时，很可能造成严重的负载不均衡．

#### 1.2.4.8. 一致性哈希算法
先构造一个长度为232的整数环（这个环被称为一致性Hash环），根据节点名称的Hash值（其分布为[0, 232-1]）将服务器节点放置在这个Hash环上，然后根据数据的Key值计算得到其Hash值（其分布也为[0, 232-1]），接着在Hash环上顺时针查找距离这个Key值的Hash值最近的服务器节点，完成Key到服务器的映射查找。
一致性hash算法还可以实现一个消费者一直命中一个服务提供者。

如下图，一共有四个服务提供者
provider-1: 127.0.0.1:8001
provider-2: 127.0.5.2:8145
provider-3: 127.0.1.2:8123
provider-4: 127.1.3.2:8256
通过hash计算后，四个节点分布在hash环的不同位置上
当有一个消费者(127.0.0.1:8011)通过hash计算后，定位到如图中所示位置，它会顺时针查找下一个节点，选择第一个查找到的节点。
![](https://img2018.cnblogs.com/blog/1404294/201904/1404294-20190418012355339-761343066.png)

**这里存在几个关键问题：**
* hash算法的影响
如果hash算法计算结果过于集中，如下图，节点分布再很小的范围内，如果消费者大部分命中范围之外，就会导致node1负载异常的大，出现负载不均衡的问题。

所以需要一个比较好的hash算法。
![](https://img2018.cnblogs.com/blog/1404294/201904/1404294-20190418013347849-699391562.png)


解决这个问题的办法是需要选择一个好的hashcode算法,hash算法比较 

* 增加或者删除节点时会导致负载不均衡
如下图：
正常情况下每个节点都是25%的命中概率
节点node2失效时，之前节点2的所有命中全部加到节点３,导致节点3的负载变大
当增加节点5时，之前节点３的命中全部给了节点５,也还是出现了负载不均衡。
![](https://img2018.cnblogs.com/blog/1404294/201904/1404294-20190418014307245-148213017.png)
解决这个问题的办法是增加虚拟节点
如下图，为每个节点都增加了虚拟节点，增加虚拟节点，可以使整个hash环分布的更加均匀，但有个问题是，节点越多，维护的性能越大，因此，需要增加多少个虚拟节点，需要根据实际需要进行测试。
![](https://img2018.cnblogs.com/blog/1404294/201904/1404294-20190418015507692-1757023041.png)

**实现**
虚拟节点的格式为　127.0.0.1:8001&&node1
分别使用jdk 的hashcode算法和FNV1_32_HASH算法进行比较。　.
```java
public class UniformityHashLoadbalanceStrategy  implements  LoadbalanceStrategy{

    private static final int VIRTUAL_NODES = 5;


    public ProviderConfig select(List<ProviderConfig> configs, Object object){

        SortedMap<Integer, ProviderConfig> sortedMap = new TreeMap();

        for(ProviderConfig config:configs){
            for(int j = 0; j < VIRTUAL_NODES; j++){
                sortedMap.put(caculHash(getKey(config.getHost(),config.getPort(),"&&node"+j)),config);
            }
        }

        System.out.println(sortedMap);
        Integer requestHashcCode = caculHash((String)object);


        SortedMap<Integer, ProviderConfig> subMap = sortedMap.subMap(requestHashcCode,Integer.MAX_VALUE);
        ProviderConfig result= null;
        if(subMap.size()  != 0){
            Integer index = subMap.firstKey();
            result =  subMap.get(index);
        }
        else{
            result = sortedMap.get(0);
        }

        ////　打印测试数据

        new PrintResult(sortedMap,requestHashcCode).print();

        /////

        return  result;


    }
    private String getKey(String host,int port,String node){
        return new StringBuilder().append(host).append(":").append(port).append(node).toString();
    }

    private int caculHash(String str){

       /* int hashCode =  str.hashCode();
        hashCode = (hashCode<0)?(-hashCode):hashCode;
        return hashCode;*/

        final int p = 16777619;
        int hash = (int)2166136261L;
        for (int i = 0; i < str.length(); i++)
            hash = (hash ^ str.charAt(i)) * p;
        hash += hash << 13;
        hash ^= hash >> 7;
        hash += hash << 3;
        hash ^= hash >> 17;
        hash += hash << 5;

        // 如果算出来的值为负数则取其绝对值
        if (hash < 0)
            hash = Math.abs(hash);
        return hash;

    }

}
//用于打印测试数据
@Data
class PrintResult{

    private  boolean flag =false;
    private SortedMap<Integer, ProviderConfig> sortedMap;
    private int requestHashcCode;

    public PrintResult(SortedMap<Integer, ProviderConfig> sortedMap, int requestHashcCode) {
        this.sortedMap = sortedMap;
        this.requestHashcCode = requestHashcCode;
    }

    public void print(){

        sortedMap.forEach((k,v)->{

            if( (false == flag) && ( k > requestHashcCode)){
                System.out.println("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++");
            }
            System.out.println("hashcode: " + k + "  " + v.getHost()+":"+v.getPort());
            if( (false == flag) && ( k > requestHashcCode)){
                System.out.println("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++");
                flag = true;
            }

        });

        System.out.println("------------------请求的hashcode:"+requestHashcCode);

    }
}
```
测试：
```java
public void uniformityHashLoadbalanceStrategyTest(LoadbalanceStrategy strategy ,int configNum){

        List<ProviderConfig> configs = new ArrayList<>();
        for(int i = 0; i< configNum; i++){
            ProviderConfig config = new ProviderConfig();
            config.setInterfaceName("com.serviceImpl");
            config.setHost("127.0.0.1");
            config.setPort(new Random().nextInt(9999));
            config.setWeight(i);
            config.setCallTime(new Random().nextInt(100));
            configs.add(config);
        }

        ProviderConfig config = strategy.select(configs,"127.0.0.1:1234");
        System.out.println("选择结果:" + config.getHost() + ":" + config.getPort());
    }
```
jdk 的　hashcode 算法

```
hashcode: 441720772  127.0.0.1:1280
hashcode: 441720773  127.0.0.1:1280
hashcode: 441720774  127.0.0.1:1280
hashcode: 441720775  127.0.0.1:1280
hashcode: 441720776  127.0.0.1:1280
hashcode: 1307619854  127.0.0.1:3501
hashcode: 1307619855  127.0.0.1:3501
hashcode: 1307619856  127.0.0.1:3501
hashcode: 1307619857  127.0.0.1:3501
hashcode: 1307619858  127.0.0.1:3501
hashcode: 1363372970  127.0.0.1:779
hashcode: 1363372971  127.0.0.1:779
hashcode: 1363372972  127.0.0.1:779
hashcode: 1363372973  127.0.0.1:779
hashcode: 1363372974  127.0.0.1:779
hashcode: 1397780469  127.0.0.1:5928
hashcode: 1397780470  127.0.0.1:5928
hashcode: 1397780471  127.0.0.1:5928
hashcode: 1397780472  127.0.0.1:5928
hashcode: 1397780473  127.0.0.1:5928
hashcode: 1700521830  127.0.0.1:4065
hashcode: 1700521831  127.0.0.1:4065
hashcode: 1700521832  127.0.0.1:4065
hashcode: 1700521833  127.0.0.1:4065
hashcode: 1700521834  127.0.0.1:4065
hashcode: 1774961903  127.0.0.1:5931
hashcode: 1774961904  127.0.0.1:5931
hashcode: 1774961905  127.0.0.1:5931
hashcode: 1774961906  127.0.0.1:5931
hashcode: 1774961907  127.0.0.1:5931
hashcode: 1814135809  127.0.0.1:5050
hashcode: 1814135810  127.0.0.1:5050
hashcode: 1814135811  127.0.0.1:5050
hashcode: 1814135812  127.0.0.1:5050
hashcode: 1814135813  127.0.0.1:5050
hashcode: 1881959435  127.0.0.1:1991
hashcode: 1881959436  127.0.0.1:1991
hashcode: 1881959437  127.0.0.1:1991
hashcode: 1881959438  127.0.0.1:1991
hashcode: 1881959439  127.0.0.1:1991
hashcode: 1889283041  127.0.0.1:4071
hashcode: 1889283042  127.0.0.1:4071
hashcode: 1889283043  127.0.0.1:4071
hashcode: 1889283044  127.0.0.1:4071
hashcode: 1889283045  127.0.0.1:4071
hashcode: 2118931362  127.0.0.1:7152
hashcode: 2118931363  127.0.0.1:7152
hashcode: 2118931364  127.0.0.1:7152
hashcode: 2118931365  127.0.0.1:7152
hashcode: 2118931366  127.0.0.1:7152
------------------请求的hashcode:35943393
选择结果:127.0.0.1:1280
```
 

可以看到ＪＤＫ默认的hashcode方法的问题，各个虚拟节点都是比较集中，会出现很严重的负载不均衡问题。

２.使用　FNV1_32_HASH算法
```
hashcode: 87760808 127.0.0.1:1926
hashcode: 127858684 127.0.0.1:2285
hashcode: 137207685 127.0.0.1:4429
hashcode: 189558739 127.0.0.1:4429
hashcode: 345597173 127.0.0.1:1926
hashcode: 411873143 127.0.0.1:5844
hashcode: 427733007 127.0.0.1:4429
hashcode: 429935214 127.0.0.1:5844
hashcode: 471059330 127.0.0.1:6013
hashcode: 508134701 127.0.0.1:6141
hashcode: 537200659 127.0.0.1:4429
hashcode: 572740331 127.0.0.1:9615
hashcode: 584730561 127.0.0.1:4429
hashcode: 586630909 127.0.0.1:6013
hashcode: 588198036 127.0.0.1:6297
hashcode: 601750027 127.0.0.1:6013
hashcode: 670864146 127.0.0.1:6297
hashcode: 823792818 127.0.0.1:9615
hashcode: 832758991 127.0.0.1:2285
hashcode: 847195135 127.0.0.1:1926
hashcode: 852642706 127.0.0.1:92
hashcode: 855431312 127.0.0.1:1926
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
hashcode: 1008339891 127.0.0.1:6430
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
hashcode: 1126143483 127.0.0.1:9615
hashcode: 1127241369 127.0.0.1:9615
hashcode: 1169946536 127.0.0.1:6297
hashcode: 1184995718 127.0.0.1:92
hashcode: 1204728048 127.0.0.1:5844
hashcode: 1218277576 127.0.0.1:2285
hashcode: 1253667665 127.0.0.1:92
hashcode: 1294893013 127.0.0.1:9615
hashcode: 1334096245 127.0.0.1:2285
hashcode: 1591823392 127.0.0.1:92
hashcode: 1597482385 127.0.0.1:6141
hashcode: 1647613853 127.0.0.1:6430
hashcode: 1653621871 127.0.0.1:6013
hashcode: 1749432497 127.0.0.1:6297
hashcode: 1765516223 127.0.0.1:92
hashcode: 1860173617 127.0.0.1:6430
hashcode: 1883591368 127.0.0.1:2285
hashcode: 1941022162 127.0.0.1:6430
hashcode: 1952262824 127.0.0.1:6141
hashcode: 1991871891 127.0.0.1:1926
hashcode: 2009814649 127.0.0.1:5844
hashcode: 2011432907 127.0.0.1:6297
hashcode: 2020508878 127.0.0.1:6141
hashcode: 2083262842 127.0.0.1:6013
hashcode: 2086348077 127.0.0.1:6141
hashcode: 2107422149 127.0.0.1:6430
hashcode: 2117355968 127.0.0.1:5844
------------------请求的hashcode:986344464
选择结果:127.0.0.1:6430
```
* 总结
    * 随机算法：
        * 好的随机算法可以使选择比较均衡，但还是会出现机器性能差异导致的调用耗时不一样。优点是实现简单。
    * 加权随机算法：
        * 可以根据不同的机器性能调整不同的权重比，从而降低机器性能差异带来的问题。
    * 轮询算法：
        * 可以使每个节点的选中概率一致，但也会出现随机算法的问题。
    * 加权轮询：
        * 可以根据不同的机器性能调整不同的权重比，从而降低机器性能差异带来的问题。
    * 最小时延算法：
        * 根据服务调用耗时动态调整，可以达到比较好的负载均衡。缺点是实现比较复杂。
    * 一致性hash算法：
        * 可以使消费者始终对应一个服务提供者。缺点是实现相对复杂。同时通过优化hashcode算法和增加虚拟节点解决分布不均的问题。

### 1.2.5. 负载均衡的实现（DNS > 数据链路层 > IP层 > Http层）
<a href="#menu" >目录</a>


#### 1.2.5.1. DNS域名解析负载均衡（延迟）
DNS域名解析负载均衡

　利用DNS处理域名解析请求的同时进行负载均衡是另一种常用的方案。在DNS服务器中配置多个A记录，如：www.mysite.com IN A 114.100.80.1、www.mysite.com IN A 114.100.80.2、www.mysite.com IN A 114.100.80.3.
　每次域名解析请求都会根据负载均衡算法计算一个不同的IP地址返回，这样A记录中配置的多个服务器就构成一个集群，并可以实现负载均衡。
　DNS域名解析负载均衡的优点是将负载均衡工作交给DNS，省略掉了网络管理的麻烦，缺点就是DNS可能缓存A记录，不受网站控制。事实上，大型网站总是部分使用DNS域名解析，作为第一级负载均衡手段，然后再在内部做第二级负载均衡。

#### 1.2.5.2. 数据链路层负载均衡(LVS)
数据链路层负载均衡(LVS)

　数据链路层负载均衡是指在通信协议的数据链路层修改mac地址进行负载均衡。
　这种数据传输方式又称作三角传输模式，负载均衡数据分发过程中不修改IP地址，只修改目的的mac地址，通过配置真实物理服务器集群所有机器虚拟IP和负载均衡服务器IP地址一样，从而达到负载均衡，这种负载均衡方式又称为直接路由方式（DR）.
　在上图中，用户请求到达负载均衡服务器后，负载均衡服务器将请求数据的目的mac地址修改为真是WEB服务器的mac地址，并不修改数据包目标IP地址，因此数据可以正常到达目标WEB服务器，该服务器在处理完数据后可以经过网管服务器而不是负载均衡服务器直接到达用户浏览器。
　使用三角传输模式的链路层负载均衡是目前大型网站所使用的最广的一种负载均衡手段。在linux平台上最好的链路层负载均衡开源产品是LVS(linux virtual server)。

#### 1.2.5.3. IP负载均衡(SNAT)
IP负载均衡
　IP负载均衡：即在网络层通过修改请求目标地址进行负载均衡。
　用户请求数据包到达负载均衡服务器后，负载均衡服务器在操作系统内核进行获取网络数据包，根据负载均衡算法计算得到一台真实的WEB服务器地址，然后将数据包的IP地址修改为真实的WEB服务器地址，不需要通过用户进程处理。真实的WEB服务器处理完毕后，相应数据包回到负载均衡服务器，负载均衡服务器再将数据包源地址修改为自身的IP地址发送给用户浏览器。
　这里的关键在于真实WEB服务器相应数据包如何返回给负载均衡服务器，一种是负载均衡服务器在修改目的IP地址的同时修改源地址，将数据包源地址改为自身的IP，即源地址转换（SNAT），另一种方案是将负载均衡服务器同时作为真实物理服务器的网关服务器，这样所有的数据都会到达负载均衡服务器。
　IP负载均衡在内核进程完成数据分发，较反向代理均衡有更好的处理性能。但由于所有请求响应的数据包都需要经过负载均衡服务器，因此负载均衡的网卡带宽成为系统的瓶颈。

#### 1.2.5.4. HTTP重定向负载均衡(少见)
HTTP重定向负载均衡
　HTTP重定向服务器是一台普通的应用服务器，其唯一的功能就是根据用户的HTTP请求计算一台真实的服务器地址，并将真实的服务器地址写入HTTP重定向响应中（响应状态吗302）返回给浏览器，然后浏览器再自动请求真实的服务器。
　这种负载均衡方案的优点是比较简单，缺点是浏览器需要每次请求两次服务器才能拿完成一次访问，性能较差；使用HTTP302响应码重定向，可能是搜索引擎判断为SEO作弊，降低搜索排名。重定向服务器自身的处理能力有可能成为瓶颈。因此这种方案在实际使用中并不见多。

#### 1.2.5.5. 反向代理负载均衡(nginx)
反向代理负载均衡
　传统代理服务器位于浏览器一端，代理浏览器将HTTP请求发送到互联网上。而反向代理服务器则位于网站机房一侧，代理网站web服务器接收http请求。
　反向代理的作用是保护网站安全，所有互联网的请求都必须经过代理服务器，相当于在web服务器和可能的网络攻击之间建立了一个屏障。
　除此之外，代理服务器也可以配置缓存加速web请求。当用户第一次访问静态内容的时候，静态内存就被缓存在反向代理服务器上，这样当其他用户访问该静态内容时，就可以直接从反向代理服务器返回，加速web请求响应速度，减轻web服务器负载压力。
　另外，反向代理服务器也可以实现负载均衡的功能。
反向代理服务器
　由于反向代理服务器转发请求在HTTP协议层面，因此也叫应用层负载均衡。优点是部署简单，缺点是可能成为系统的瓶颈。

## 1.3. 隔离
<a href="#menu" >目录</a>

### 1.3.1. 概述
* 隔离是将系统或者资源分隔开，系统隔离是为了某个系统发生故障或者业务发生故障时，尽量减少影响面。保证其他服务或者业务能够继续运行。

* 线程隔离
    * 使用线程池，某一个线程出现故障时，不会影响其他线程。
* 进程隔离
* 集群隔离
* 机房隔离
    * 为了提高可用性，进行多机房部署，每个机房都会有自己的服务分组
    * 本机房的服务应该只调用本机房的服务，不进行跨机房调用
    * 当一个机房发生问题时，可以通过DNS负载均衡将请求全部切换到另一个机房，或者考虑服务能够重试其他机房的服务。
* 读写隔离
* 动静隔离
    * 将动态内容和静态资源分离
    * 一般将静态资源放在CDN上
* 爬虫隔离
    * 限流
    * 识别，路由到单独集群
* 热点隔离
    * 比如秒杀服务单独部署
* 资源隔离
    * 磁盘，CPU，网络等会存在竞争
    * 不同需求的应用部署在不同的硬件环境上
* 环境隔离
    * 测试环境，预发布环境，灰度环境，正式环境
* 压测隔离
    * 真实数据，压测数据隔离
    * AB测试，为不同的用户提供不同版本的服务
* 缓存隔离
    * 不同的应用使用不同得到缓存服务器
* 查询隔离
    * 简单，复杂，批量查询分别路由到不同的集群

## 1.4. 限流
<a href="#menu" >目录</a>

### 1.4.1. 概述
<a href="#menu" >目录</a>
* 限流的目的是通过对并发访问的请求进行限速或者对于一定窗口内的请求进行限速，一旦达到系统的限制值就可以拒绝服务(定向错误页，返回错误通知，排队，降级)。
* 可以通过压测来测试系统的处理峰值
* 也可以根据系统的吞吐量，响应时间，可用率来动态调整限流峰值
* 常见的限流策略
    * 限制总并发数
        * 数据库连接出，线程池
    * 限制瞬时并发数
        * Nginx的limit_conn模块
    * 限制时间窗口内的平均速率
        * Guava的RateLimiter ,Nginx的limit_req 
    * 限制远程接口的调用速率
    * 限制MQ的消费速率
    * 还可以根据网络连接数，网络流量，CPU或内存负载等来限流  


### 1.4.2. 限流算法


#### 1.4.2.1. 计数器法
计数器法是限流算法里最简单也是最容易实现的一种算法。比如我们规定，对于A接口来说，我们1分钟的访问次数不能超过100个。那么我们可以这么做：在一开 始的时候，我们可以设置一个计数器counter，每当一个请求过来的时候，counter就加1，如果counter的值大于100并且该请求与第一个 请求的间隔时间还在1分钟之内，那么说明请求数过多；如果该请求与第一个请求的间隔时间大于1分钟，且counter的值还在限流范围内，那么就重置 counter

#### 1.4.2.2. 滑动窗口
滑动窗口，又称rolling window。为了解决这个问题，我们引入了滑动窗口算法。如果学过TCP网络协议的话，那么一定对滑动窗口这个名词不会陌生。下面这张图，很好地解释了滑动窗口算法：

在上图中，整个红色的矩形框表示一个时间窗口，在我们的例子中，一个时间窗口就是一分钟。然后我们将时间窗口进行划分，比如图中，我们就将滑动窗口 划成了6格，所以每格代表的是10秒钟。每过10秒钟，我们的时间窗口就会往右滑动一格。每一个格子都有自己独立的计数器counter，比如当一个请求 在0:35秒的时候到达，那么0:30~0:39对应的counter就会加1。

那么滑动窗口怎么解决刚才的临界问题的呢？我们可以看上图，0:59到达的100个请求会落在灰色的格子中，而1:00到达的请求会落在橘黄色的格 子中。当时间到达1:00时，我们的窗口会往右移动一格，那么此时时间窗口内的总请求数量一共是200个，超过了限定的100个，所以此时能够检测出来触 发了限流。

我再来回顾一下刚才的计数器算法，我们可以发现，计数器算法其实就是滑动窗口算法。只是它没有对时间窗口做进一步地划分，所以只有1格。

由此可见，当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。

* 计数器 VS 滑动窗口
计数器算法是最简单的算法，可以看成是滑动窗口的低精度实现。滑动窗口由于需要存储多份的计数器（每一个格子存一份），所以滑动窗口在实现上需要更多的存储空间。也就是说，如果滑动窗口的精度越高，需要的存储空间就越大。

#### 1.4.2.3. 漏桶算法
<a href="#menu" >目录</a>
<a href="#menu" >目录</a>
漏桶(Leaky Bucket)算法思路很简单，水（请求）先进入到漏桶里，漏桶以一定的速度出水（接口有响应速率），当水流入速度过大会直接溢出（访问频率超过接口响应速率），然后就拒绝请求，可以看出漏桶算法能强行限制数据的传输速率。示意图如下：
![](https://img2018.cnblogs.com/blog/1136672/201904/1136672-20190421202927762-1718486905.png)

#### 1.4.2.4. 令牌桶算法
<a href="#menu" >目录</a>

令牌桶算法（Token Bucket）和 Leaky Bucket 效果一样但方向相反的算法，更加容易理解。随着时间流逝，系统会按恒定1/QPS时间间隔（如果QPS=100，则间隔是10ms）往桶里加入Token（想象和漏洞漏水相反，有个水龙头在不断的加水），如果桶已经满了就不再加了。新请求来临时，会各自拿走一个Token，如果没有Token可拿了就阻塞或者拒绝服务。示意图如下：
![](https://img2018.cnblogs.com/blog/1136672/201904/1136672-20190421202936084-459487536.jpg)

漏桶算法与令牌桶算法的区别在于，漏桶算法能够强行限制数据的传输速率，令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的突发传输。令牌桶的另外一个好处是可以方便的改变速度。 一旦需要提高速率，则按需提高放入桶中的令牌的速率。一般会定时（比如100毫秒）往桶中增加一定数量的令牌, 有些变种算法则实时的计算应该增加的令牌的数量。

### 1.4.3. 应用级���流
<a href="#menu" >目录</a>

#### 1.4.3.1. 限流总并发数/连接/请求数

#### 1.4.3.2. 限流总资源数

#### 1.4.3.3. 限流某个接口的总并发数/请求数

#### 1.4.3.4. 限流某个接口的时间窗请求数

#### 1.4.3.5. 平滑限流某个接口的请求数





### 1.4.4. 分布式限流
<a href="#menu" >目录</a>

#### 1.4.4.1. Redis与Lua

案例-实现访问频率限制: 实现访问者 $ip 在一定的时间 $time 内只能访问 $limit 次.
非脚本实现
```java
private boolean accessLimit(String ip, int limit, int time, Jedis jedis) {
    boolean result = true;

    String key = "rate.limit:" + ip;
    if (jedis.exists(key)) {
        long afterValue = jedis.incr(key);
        if (afterValue > limit) {
            result = false;
        }
    } else {
        Transaction transaction = jedis.multi();
        transaction.incr(key);
        transaction.expire(key, time);
        transaction.exec();
    }
    return result;
}
```

以上代码有两点缺陷 
可能会出现竞态条件: 解决方法是用 WATCH 监控 rate.limit:$IP 的变动, 但较为麻烦;
以上代码在不使用 pipeline 的情况下最多需要向Redis请求5条指令, 传输过多.
Lua脚本实现 
Redis 允许将 Lua 脚本传到 Redis 服务器中执行, 脚本内可以调用大部分 Redis 命令, 且 Redis 保证脚本的原子性:

首先需要准备Lua代码: script.lua
```lua
local key = "rate.limit:" .. KEYS[1]
local limit = tonumber(ARGV[1])
local expire_time = ARGV[2]

local is_exists = redis.call("EXISTS", key)
if is_exists == 1 then
    if redis.call("INCR", key) > limit then
        return 0
    else
        return 1
    end
else
    redis.call("SET", key, 1)
    redis.call("EXPIRE", key, expire_time)
    return 1
end
```

Java
```java
private boolean accessLimit(String ip, int limit, int timeout, Jedis connection) throws IOException {
    List<String> keys = Collections.singletonList(ip);
    List<String> argv = Arrays.asList(String.valueOf(limit), String.valueOf(timeout));

    return 1 == (long) connection.eval(loadScriptString("script.lua"), keys, argv);
}

// 加载Lua代码
private String loadScriptString(String fileName) throws IOException {
    Reader reader = new InputStreamReader(Client.class.getClassLoader().getResourceAsStream(fileName));
    return CharStreams.toString(reader);
}
```

Lua 嵌入 Redis 优势: 
减少网络开销: 不使用 Lua 的代码需要向 Redis 发送多次请求, 而脚本只需一次即可, 减少网络传输;
原子操作: Redis 将整个脚本作为一个原子执行, 无需担心并发, 也就无需事务;
复用: 脚本会永久保存 Redis 中, 其他客户端可继续使用.


#### 1.4.4.2. Nginx

nginx本身有限流模块


## 1.5. 降级
<a href="#menu" >目录</a>

### 1.5.1. 降级概念

**服务降级**
当服务器压力剧增的时候,根据当前业务情况以及流量，对一些服务和页面有策略的降级，以此缓解服务器资源的压力以保障核心任务的正常运行，同时也保证了大部分客户能得到正常的响应。
* 服务接口拒绝服务：页面能访问，但是添加删除提示服务器繁忙。页面内容也可在Varnish或CDN内获取。
* 页面拒绝服务：页面提示由于服务繁忙此服务暂停。跳转到varnish或nginx的一个静态页面。
* 延迟持久化：页面访问照常，但是涉及记录变更，会提示稍晚能看到结果，将数据记录到异步队列或log，服务恢复后执行。
* 随机拒绝服务：服务接口随机拒绝服务，让用户重试，目前较少有人采用。因为用户体验不佳。

  在一般稍微大一点的互联网公司基本上都会有一个配置中心的角色,通常由配置服务和代理和应用程序组成，Agent会定期的或者实时的接受配置中心的变更，将配置信息写入本地文件。此时SDK会同步代理的配置以达到同步配置中心的数据。当然也可以没有Agent这一角色，SDK直接监听配置中心的变更。
  拥有了这一架构之后,对于每个应用程序的请求或者数据库都可以通过配置中心来进行降级与切换。当然了如果目前所处环境没有这一条件也可以使用单纯的DB来保存 key-value来简易实现这一功能。

**服务熔断(过载保护)**
  对于炒股的同学，熔断这个词可能并不陌生，它是指当某一股值波浮达到某一个点后交易所为了控制风险，采取一些暂停交易的措施。响应的如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。
  三个模块：熔断请求判断算法、熔断恢复机制、熔断报警
（1）熔断请求判断机制算法：使用无锁循环队列计数，每个熔断器默认维护10个bucket，每1秒一个bucket，每个blucket记录请求的成功、失败、超时、拒绝的状态，默认错误超过50%且10秒内超过20个请求进行中断拦截。
（2）熔断恢复：对于被熔断的请求，每隔5s允许部分请求通过，若请求都是健康的（RT<250ms）则对请求健康恢复。
（3）熔断报警：对于熔断的请求打日志，异常请求超过某些设定则报警

**降级与熔断对比**

* **共性**
    * 目的: 目的一致，都是从系统的可用性、可靠性着想。放了防止系统的整体缓慢甚至奔溃而采用的技术手段。
    * 最终表现: 表现类似,最终都是给用户一种当前服务不可用或者不可达的感觉
    * 粒度: 大多都是在服务级别，当然也有一些在持久层层面的应用
    * 自治: 基本都是靠系统达到某一临界条件时，实现自动的降级与熔断，人工降级并不是那么稳妥。

* **区别**
    * 触发原因: 服务熔断一般指某个服务的下游服务出现问题时采用的手段,而服务降级一般是从整体层面考虑的。
    * 管理目标层次: 熔断是一种框架级的处理，每一个微服务都需要。而降级一般需要对业务有层级之分，降级一般都是从外围服务开始的。
    * 实现方式: 代码级别实现有差异

**服务降级需要考虑的问题**
* 核心服务、非核心服务
* 是否支持降级，降级策略
* 业务放通场景、策略

**使用场景**
当整个微服务架构整体的负载超出了预设的上限阈值或即将到来的流量预计将会超过预设的阈值时，为了保证重要或基本的服务能正常运行，我们可以将一些不重要或不紧急的服务或任务进行服务的延迟使用或暂停使用。



* 降级的最终目的是保证核心服务可用，降级也是要根据系统的吞吐量，响应时间，可用率等条件进行手动降级或者自动降级。
* 降级等级分类
    * 一般
        * 比如服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级
    * 警告
        * 有些服务在一段时间内成功率有波动，可以自动降级或者人工降级，并发送警告
    * 错误
        * 比如可用率降低，访问量猛增超过系统阈值
    * 严重错误
* 按照自动化分类
    * 自动开关降级
        * 超时降级
            * 响应缓慢的时候自动降级
        * 统计失败次数降级
            * 失败后进行尝试，尝试失败多次则进行降级
        * 故障降级
            * 出现故障时降级，比如RPC调用失败
        * 限流降级
            * 并发大的情况下，超过限流值时进行降级，比如拒绝服务。
    * 人工降级
        * 通过监控，发现CPU，内存等出现异常，则手动进行降级
* 功能区分
    * 读服务降级
        * 后端数据库等存储出现不可用，更换为读端存，仅适用于数据一致性要求不高的场景
    * 写服务降级
        * 并发大的情况下，写数据库会出现问题，先将数据读到缓存，在缓存里面进行操作，需要保证最终的数据一致性
* 系统层次区分
    * 多级降级
* 降级处理
    * 页面降级
    * 页面片段降级
    * 页面异步请求降级
    * 服务功能降级
    * 读降级
    * 服务降级
    * 爬虫降级
    * 风控降级

### 1.5.2. 使用Hystrix实现降级
<a href="#menu" >目录</a>

#### 1.5.2.1. 降级Demo
话不多说，下面先来一个Demo（对于Hystrix的依赖，这里就不再介绍了）。

**GetStockServiceCommand**
```java
public class GetStockServiceCommand extends HystrixCommand<String> {

    private StockService stockService;

    public GetStockServiceCommand (StockService stockService) {
        super(setter());
        this.stockService = stockService;
    }

    private static Setter setter() {
        // 服务分组
        HystrixCommandGroupKey groupKey = HystrixCommandGroupKey.Factory.asKey("stock");
        // 命令配置
        HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter()
                .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD)
                .withFallbackEnabled(true)
                .withFallbackIsolationSemaphoreMaxConcurrentRequests(100)
                .withExecutionIsolationThreadInterruptOnFutureCancel(true)
                .withExecutionIsolationThreadInterruptOnTimeout(true)
                .withExecutionTimeoutEnabled(true)
                .withExecutionTimeoutInMilliseconds(1000);
        return HystrixCommand.Setter.withGroupKey(groupKey).andCommandPropertiesDefaults(commandProperties);
    }

    @Override
    protected String run() throws Exception {
        // 可以通过异常/Thread.sleep()模拟超时
        return stockService.getStock();
    }

    @Override
    protected String getFallback() {
        return "有货";
    }
}
```

**StockService**
```java
public class StockService {

    public String getStock() {
        throw new RuntimeException("出现异常了!");
    }
}
```

**GetStockTest**
```java
public class GetStockTest {

    public static void main(String[] args) {
        StockService stockService = new StockService();
        GetStockServiceCommand stockServiceCommand = new GetStockServiceCommand(stockService);
        String result = stockServiceCommand.execute();// 同步执行
        System.out.println(result);

        // 异步调用, 可自由控制获取结果时机
        // Future<String> future = helloworldCommand.queue();
        // get操作不能超过command定义的超时时间, 默认1秒
        // result = future.get(100, TimeUnit.MILLISECONDS);
    }
}
```

GetStockTest为测试入口，为了对stockService服务做自动降级。对stockService做了一层Command包装，然后调用execute()去执行GetStockServiceCommand里面的run()方法。因为stockService.getStock()方法抛出了异常，所以会执行降级操作，也就是getFallback()方法会被执行并返回。

#### 1.5.2.2. 降级参数

**使用HystrixCommandProperties配置和getFallback()方法可以实现降级处理。下面详细介绍一下配置参数：**
* withFallbackEnabled：是否启用降级，若启用，则在超时或异常时调用getFallback进行降级。（默认开启）
* withFallbackIsolationSemaphoreMaxConcurrentRequests：配置了fallback()请求并发的信号量，当调用fallback()的并发超过阀值（默认10），则会进入快速失败。
* withExecutionIsolationThreadInterruptOnFutureCancel：当隔离策略为THREAD时，当线程执行超时，是否进行中断处理，即异步的Future#cancel()。（默认为false）
* withExecutionIsolationThreadInterruptOnTimeout：当隔离策略为THREAD时，当线程执行超时，是否进行中断处理。（默认为true）这里指的是同步调用：execute()
* withExecutionTimeoutEnabled：是否启用超时机制，默认为true。
* withExecutionTimeoutInMilliseconds：执行超时时间，默认1000毫秒。1、配置线程隔离，则执行中断处理；2、配置信号量隔离，则进行终止操作。因为信号量隔离和主线程是在一个线程中执行，其不会中断线程处理。所以要根据实际情况选择类型。

**除了上面的部分参数，对于getFallback()还需要注意以下的几点：**
最大并发数受fallbackIsolationSemaphoreMaxConcurrentRequests控制，如果失败率非常高，则需要重新配置该参数。如果并发数超过了该配置，则不会再执行getFallback()，而是快速失败。如抛出HystrixRuntimeException的异常。
该方法不能进行网络调用，应该只是返回兜底的数据。
如果必须要走一个网络调用，则就需要调用另外一个Command。
Command可以有降级和熔断机制，而getFallback只有fallbackIsolationSemaphoreMaxConcurrentRequest参数控制最大并发数。

#### 1.5.2.3. 熔断
Command首先调用HystrixCircuitBreaker#allowRequest判断是否熔断了，如果没有则执行Command#run方法；若熔断了则直接调用Command#getFallback方法降级处理。

通过circuitBreakerSleepWindowInMilliseconds可以控制一个时间窗口内，可进行一次请求测试。若测试成功，则闭合熔断开关，否则还是打开状态，从而实现了快速失败和恢复。关于熔断有以下几个概念需要了解一下：

**概念**
* 闭合（Closed）：如果配置了熔断开关强制闭合，或者当前请求失败率没有超过阀值，则熔断开关处于闭合状态，此时不会启动熔断机制，即不进行降级处理。
* 打开（Open）：如果配置了熔断开关强制打开，或者当前失败率超过了阀值，则熔断开关打开，此时会调用getFallback()方法进行降级处理。
* 半打开（Half-Open）：当熔断处于打开状态后，不能一直熔断下去，需要在一个时间窗口之后进行重试，这就是半打开状态。Hystrix允许在circuitBreakerSleepWindowInMilliseconds的时间窗口内进行一次重试。重试成功后闭合熔断开关，否则熔断开关还是处于打开状态
。
上面所指的失败包含：异常、超时、线程池拒绝、信号量拒绝的总和。

**配置示例**
```java
HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter()
    .withCircuitBreakerEnabled(true)// 默认为true
    .withCircuitBreakerForceClosed(false)// 默认为false
    .withCircuitBreakerForceOpen(false)// 默认为false
    .withCircuitBreakerErrorThresholdPercentage(50)// 默认50%
    .withCircuitBreakerRequestVolumeThreshold(20)// 默认为20
    .withCircuitBreakerSleepWindowInMilliseconds(5000)// 默认5秒
```
* withCircuitBreakerEnabled：是否开启熔断机制，默认为true。
* withCircuitBreakerForceClosed：是否强制关闭熔断开关，如果强制关闭了熔断开关，则请求不会被降级，一些场景可以动态设置该开关，默认为false。
* withCircuitBreakerForceOpen：是否强制打开熔断开关，如果打开了，则请求强制降级调用getFallback处理，可以通过动态配置来打开开关实现一些特殊需求，默认为false。
* withCircuitBreakerErrorThresholdPercentage：如果在一个采样时间窗口内，失败率超过该配置，则自动打开熔断开关，快速失败。默认采样周期为10秒，失败率为50%。
* withCircuitBreakerRequestVolumeThreshold：在熔断开关闭合的情况下，在进行失败率判断之前，一个采样周期内必须进行至少N个请求才能进行采样统计。目的是有足够的采样使得失败率计算的比较接近真实值，默认为20.
* withCircuirBreakerSleepWindowInMilliseconds：熔断后的重试时间窗口，在窗口内只允许一次重试。在熔断开关打开后，若重试成功，则重试Health采样统计，并闭合熔断开关实现快速恢复。否则熔断开关还是打开状态，会进行快速失败。


**通过下面的方法可以获取熔断器的状态：**
* isCircuitBreakerOpen：熔断开关是否打开了，通过 circuitBreakerForceOpen().get() || (!circuitBreakerForceClosed().get() && circuitBreaker.isOpen()) 判断。
* isResponseShortCircuited：isCircuitBreakerOpen=true，且调用getFallback()时返回true。

#### 1.5.2.4. 采样统计

**Hystrix在内存中存储采样数据，支持如下3种采样：**
* BucketedCounterStream：计数统计。记录一定时间窗口内的失败、超时、线程池拒绝、信号量拒绝数量。写入第N组时，用前N-1组统计，然后基于时间窗口平滑后移统计。
![](https://img-blog.csdn.net/20180713101312536?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eGlhbjkw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
* RollingConcurrencyStream：最大并发数统计。如Command/ThreadPool的最大并发数。
* RollingDistributionStream：延迟百分比统计，和HystrixRollingNumber类似，差别在于其是百分位数的���计。比如每组记录P（如100）个数值，统计时用前N-1组数据，将分组数据按从小到大排序，然后累加，处于p%位置的就是p百分位数，通过它可以实现P50、P99、P999，Hystrix用来统计延时的分布情况。

**1、Command、ThreadPool计数/最大并发采样统计**
```java
HystrixThreadPoolProperties.Setter threadPoolProperties = HystrixThreadPoolProperties.Setter()
    .withMetricsRollingStatisticalWindowInMilliseconds(1000)
    .withMetricsRollingStatisticalWindowBuckets(10);
HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter()
    .withMetricsRollingStatisticalWindowInMilliseconds(10000)
    .withMetricsRollingStatisticalWindowBuckets(10);
```

* withMetricsRollingStatisticalWindowInMilliseconds：配置采样统计滚转之间窗口，默认为10秒。
* withMetricsRollingStatisticalWindowBuckets：配置采用统计滚转时间窗口内的桶的总数量，默认为10，比如时间窗口为10000，桶数量为10，则采用统计间隔为每秒一个桶统计。

**2、Command健康度采样统计**
```java
HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter()
    .withMetricsRollingStatisticalWindowInMilliseconds(10000)
    .withMetricsHealthSnapshotIntervalInMilliseconds(500);
```
* withMetricsRollingStatisticalWindowInMilliseconds：同上。
* withMetricsHealthSnapshotIntervalInMilliseconds：记录健康度采用统计的快照频率，默认为500ms，即500ms一个采样统计间隔，那么桶的数量为10000/500=20个。
该统计在熔断机制中使用时，如果计算熔断的频率非常高，则需要控制好采样的频率。如果太频繁，就有可能造成CPU计算密集。所以选择Hystrix要注意此处的性能消耗和调优，如果此处是瓶颈，则可以费除掉统计。

**3、Command时延分布采样统计**
```java
HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter()
    .withMetricsRollingPercentileWindowInMilliseconds(60000)
    .withMetricsRollingPercentileWindowBuckets(6);

上面默认采样滚转时间窗口为60S，有6个桶，即每10S一个桶统计。

```

Hystrix流程结构
![](https://img-blog.csdn.net/20180713101455796?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eGlhbjkw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**流程说明**
* 1、每次调用创建一个新的HystrixCommand，把依赖调用封装在run()方法中。 
* 2、执行execute/queue做同步或异步调用。 
* 3、判断熔断器(circuit-breaker)是否打开，如果打开跳到步骤8，进行降级策略，如果关闭进入步骤4。 
* 4、判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8，否则继续后续步骤。 
* 5、调用HystrixCommand的run方法，运行依赖逻辑。 
* 5a、依赖逻辑调用超时，进入步骤8。 
* 6、判断逻辑是否调用成功。 
* 6a、返回成功调用结果。 
* 6b、调用出错，进入步骤8。 
* 7、计算熔断器状态，所有的运行状态(成功、失败、拒绝、超时)上报给熔断器，用于统计从而判断熔断器状态。 
* 8、getFallback()降级逻辑。
    * 以下四种情况将触发getFallback调用：
        * (1) run()方法抛出非HystrixBadRequestException异常
        * (2) run()方法调用超时
        * (3) 熔断器开启拦截调用 
        * (4) 线程池/队列/信号量是否跑满
* 8a、没有实现getFallback的Command将直接抛出异常。 
* 8b、fallback降级逻辑调用成功直接返回。 
* 8c、降级逻辑调用失败抛出异常。 
* 9、返回执行成功结果。

#### 1.5.2.5. 线程/信号量隔离

**线程隔离**
把执行依赖代码的线程与请求线程分离，请求线程可以自由控制离开的时间(异步过程)。通过线程池大小可以控制并发量，当线程池饱和时可以提前拒绝服务，防止依赖问题扩散。线上建议线程池不要设置过大，否则大量堵塞线程有可能会拖慢服务器。

**信号量隔离**
信号隔离也可以用于限制并发访问，防止阻塞扩散，与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请），如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离，降低开销。


## 1.6. 回滚机制
<a href="#menu" >目录</a>

### 1.6.1. 事务回滚
* 事务回滚是为了防止出现数据不一致的问题。
* 对于单库回滚，数据库支持单库回滚
* 分布式事务方案
    * 强一致性
        * 两阶段提交
        * 三阶段协议
        * 这两种实现回滚难度较低，但是对性能影响较大
    * 最终一致性实现
        * 事务表
        * 消息队列
        * 补偿机制（执行/回滚）
        * TCC模式（预占/确认/消息）
        * Sagas（拆分事务/补偿机制）
### 1.6.2. 代码库回滚
* Git
* SVN
### 1.6.3. 部署版本回滚
* 部署版本化
    * 发布时全量发布，避免增量发布（只发布修改过的类），全版本可以直接回滚，不会受到约束或限制。
* 小版本增量发布
* 大版本灰度发布
    * 两个版本同时发布，一些用户访问老版本，一些用户访问新版本
    * 不同版本就是不同的服务，在一套集群内部署
    * 运行一段时间后没问题再全量发布
* 架构升级
    * 在nginx层面慢慢将流量路由到新版本，直到100%
    * 如中间出现故障，可立即切换到旧版本
### 1.6.4. 静态资源回滚

## 1.7. 压测与预案
<a href="#menu" >目录</a>

一般通过系统压测发现系统瓶颈和问题，然后进行系统优化啊和容灾，进而提升系统的健壮性和处理能力。

* TP=Top Percentile，Top百分数，是一个统计学里的术语，与平均数、中位数都是一类。
    * TP50、TP90和TP99等指标常用于系统性能监控场景，指高于50%、90%、99%等百分线的情况。
    * TP50：指在一个时间段内（如5分钟），统计该方法每次调用所消耗的时间，并将这些时间按从小到大的顺序进行排序，取第50%的那个值作为TP50的值；配置此监控指标对应的报警阀值后，需要保证在这个时间段内该方法所有调用的消耗时间至少有50%的值要小于此阀值，否则系统将会报警
    * TP90，TP99，TP999与TP50值计算方式一致，它们分别代表着对方法的不同性能要求，TP50相对较低，TP90则比较高，TP99，TP999则对方法性能要求很高
    
* 在系统的高可靠性（也称为可用性，英文描述为HA，High Available）里有个衡量其可靠性的标准——X个9，这个X是代表数字3~5。X个9表示在系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比，我们通过下面的计算来感受下X个9在不同级别的可靠性差异。
    * 3个9：(1-99.9%)*365*24=8.76小时，表示该系统在连续运行1年时间里最多可能的业务中断时间是8.76小时。
    * 4个9：(1-99.99%)*365*24=0.876小时=52.6分钟，表示该系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟。
    * 5个9：(1-99.999%)*365*24*60=5.26分钟，表示该系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟
### 1.7.1. 系统压测
* 压测方案
    * 压测接口
    * 并发量
    * 压测策略(突发,逐步加压,并发量)
* 压测指标
    * 机器负载
    * QPS/TPS
    * 响应时间(平均，最小，最大)
* 压测报告
    * 相关参数以及测试结果
#### 1.7.1.1. 线下压测
* 线下通过Jmeter或者Apache ab压测系统的某个接口，然后进行调优。以达到组件性能最优状态
* 线下压测环境和线上环境(服务器，网络，数据量)和线上完全不一样，因此测试结果只能作为参考

#### 1.7.1.2. 线上压测
* 读写区分
    * 读压测
    * 写压测
    * 混合压测
* 数据仿真度
    * 仿真压测
    * 引流压测
* 是否给用户提供服务
    * 隔离集群压测
    * 线上集群压测
    * 单机压测
* 压测可靠性保证
    * 数据离散化，比如分库分表情况下，避免压测的数据都是路由到同一个数据库
    * 全链路压测，对所有的服务进行压测

### 1.7.2. 系统优化和容灾
* 很据压测报告进行相应的优化和升级，比如硬件升级，集群扩容，参数优化，代码优化




## 1.8. 缓存
<a href="#menu" >目录</a>
* 缓存命中率
    * 缓存命中的次数/缓存查询次数
    * 命中率越高越好
    * 通过监控该参数确认是否工作良好

### 1.8.1. 应用级缓存
<a href="#menu" >目录</a>

#### 1.8.1.1. 缓存命中率
缓存命中率是指缓存读取成功与总读取次数之比，越高越好，应该通过这个数据监控缓存是否工作良好。


#### 1.8.1.2. 缓存回收策略
<a href="#menu" >目录</a>

* 基于空间
    * 占用的存储空间大小
* 基于容量
    * 缓存的数量
* 基于时间
    * 缓存的存在时间
* 基于Java对象引用
    * 软引用
    * 弱引用
    
#### 1.8.1.3. 回收算法
<a href="#menu" >目录</a>

##### 1.8.1.3.1. FIFO
* FIFO ：（First In First Out）：先进先出算法，即先放入缓存的先被移除。
* 存在的问题
    * 当大量的新缓存插入会使早期进入的热点缓存会被移除掉。
##### 1.8.1.3.2. LRU 
* LRU（Least Recently Used）：最近最少使用算法，使用时间距离现在最久的那个被移除。
* 实现
    * 当有新数据时插入链表头部
    * 当缓存命中，则将数据移到链表头部
    * 当链表满的时候，移除链表尾部的数据
* 存在的问题
    * 如果最近一段时间没有访问热点缓存，访问的是冷数据，热点缓存会被移除掉

##### 1.8.1.3.3. LFU
* LFU（Least Frequently Used）：最不常用算法，一定时间段内使用【次数（频率）】最少的那个被移除。
* 给每一个缓存添加访问计数器，缓存不足时移除计数器最小的缓存
* 存在的问题
    * 如果频率时间度量是1小时，则平均一天每个小时内的访问频率1000的热点数据可能会被2个小时的一段时间内的访问频率是1001的数据剔除掉；
    *  最近新加入的数据总会易于被剔除掉，由于其起始的频率值低。本质上其“重要性”指标访问频率是指在多长的时间维度进行衡量？其难以标定，所以在业界很少单一直接使用。也由于两种算法的各自特点及缺点，所以通常在生产线上会根据业务场景联合LRU与LFU一起使用，称之为LRFU。
##### 1.8.1.3.4. LRFU
* 利用两个队列维护访问的数据元素，按被访问的频率的维度把元素分别搁在热端与冷端队列；而在同一个队列内，最后访问时间越久的元素会越被排在队列尾。
 
#### 1.8.1.4. Java 缓存类型
<a href="#menu" >目录</a>

##### 1.8.1.4.1. 堆缓存
* 使用堆内存来存储对象
* 好处是不需要序列化和反序列化，速度快。
* 当缓存比较大时，GC回收时间比较长
* 一般通过软引用/弱引用来存储对象,即当堆内存不足时，可以强制回收这部分内存。
* 一般用于缓存少量的热点数据，并且不是频繁修改的，因为集群环境下会出现数据不一致问题，需要做好过期时间设置
* 常用实现方案有: Guava ,Ehcache ,MapDB

##### 1.8.1.4.2. 堆外缓存
* 使用堆外内存进行缓存,减少GC暂停时间
* 可以使用更大的缓存空间，受机器内存限制
* 实现方案:Ehcache ,MapDB

##### 1.8.1.4.3. 磁盘缓存
* 存储磁盘，重启后仍可以加载缓存
* 实现方案:Ehcache ,MapDB

##### 1.8.1.4.4. 分布式缓存
* 实现多应用共享缓存
* 实现方案:Redis

##### 1.8.1.4.5. 多级缓存
多级缓存就是根据不同的访问速率来设置多级缓存。优先访问速率高的缓存，提升系统性能。
比如先访问本地缓存，本地缓存不存在，再访问分布式缓存。可以尽量减少一次网络操作。

#### 1.8.1.5. 应用级缓存示例
<a href="#menu" >目录</a>

* 设计策略
    * 统一API封装
    * 可选的缓存方案
    * 失败统计以提供系统监控和分析
    * 命中率低通知报警
    * 缓存一致性考虑


#### 1.8.1.6. 缓存使用模式实践
<a href="#menu" >目录</a>

* SOR
    * 记录系统，实际存储原始数据的系统，比如数据库
* Cache
    * 缓存，访问速度比SOR快
* 回源
    * 缓存没有命中，回源数据库拿数据

##### 1.8.1.6.1. Cache-Aside
* 业务代码维护缓存，也就是业务代码和缓存操作混在一起
* 并发更新问题，多个实例同时更新
    * 如果是用户维度的问题，比如用户的订单数据、用户数据，并发更新的情况很少，加上过去时间就可以
    * 对于商品数据，可以考虑canal订阅binlog.来进行增量更新，不会出现不一致情况，但会存在延迟
    * 
##### 1.8.1.6.2. Cache-As-SOR
* Cache 看作SOR，所有操作都是对Cache进行，然后Cache再委托给SOR进行真实的读写，即代码中只看到Cache的操作
* 有三种实现: Read-Through, Write-Through,Write-Behind

##### 1.8.1.6.3. Read-Through
* 先查询缓存，缓存不命中再回源SOR，而不是业务代码进行回源。比如Guava的失败回调
* 优点:业务代码更加简洁
* 缺点：不适合复杂的查询，因为每次回源的查询条件 是不一样的，需要根据每个查询单独编写代码，可以使用回调函数解决

##### 1.8.1.6.4. Write-Through
* 穿透写模式/直写模式
* 业务代码首先调用Cache写数据，然后由Cache负责写缓存和写Sor,而不是由业务代码操作

##### 1.8.1.6.5. Write-Behind
* 回写模式,异步操作，异步之后可以实现批量写，合并写，延时写和限流

##### 1.8.1.6.6. Copy-Pattern
* 两种复制模式，Copy on read,copy on write
* Guava Cache 和Ehcache中堆缓存都是基于引用，有可能发生有人拿到缓存后修改，导致数据出现修改问题。
* Ehcache3.x提供解决方案

#### 1.8.1.7. 缓存一致性处理
<a href="#menu" >目录</a>

先做一个说明，从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。

在这里，我们讨论三种更新策略：
* 先更新数据库，再更新缓存
* 先删除缓存，再更新数据库
* 先更新数据库，再删除缓存

**先更新数据库，再更新缓存**

这套方案，大家是普遍反对的。为什么呢？有如下两点原因。

* 原因一（线程安全角度）
    * 同时有请求A和请求B进行更新操作，那么会出现
        * 线程A更新了数据库
        * 线程B更新了数据库
        * 线程B更新了缓存
        * 线程A更新了缓存

这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。

* 原因二（业务场景角度）
    * 有如下两点：
        * 如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。
        * 如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。

**先删缓存，再更新数据库**

该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:
* 请求A进行写操作，删除缓存
* 请求B查询发现缓存不存在
* 请求B去数据库查询得到旧值
* 请求B将旧值写入缓存
* 请求A将新值写入数据库

上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。

那么，如何解决呢？采用延时双删策略
伪代码如下
```java
public void write(String key,Object data){
    redis.delKey(key);
    db.updateData(data);
    Thread.sleep(1000);
    redis.delKey(key);
}
```
转化为中文描述就是
* 先淘汰缓存
* 再写数据库（这两步和原来一样）
* 休眠1秒，再次淘汰缓存

这么做，可以将1秒内所造成的缓存脏数据，再次删除。那么，这个1秒怎么确定的，具体该休眠多久呢？针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。

如果你用了mysql的读写分离架构怎么办？
ok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。
* 请求A进行写操作，删除缓存
* 请求A将数据写入数据库了，
* 请求B查询缓存发现，缓存没有值
* 请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值
* 请求B将旧值写入缓存
* 数据库完成主从同步，从库变为新值

上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。

采用这种同步淘汰策略，吞吐量降低怎么办？
ok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。

第二次删除,如果删除失败怎么办？
这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库：
* 请求A进行写操作，删除缓存
* 请求B查询发现缓存不存在
* 请求B去数据库查询得到旧值
* 请求B将旧值写入缓存
* 请求A将新值写入数据库
* 请求A试图去删除请求B写入对缓存值，结果失败了。

ok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。
如何解决呢？
具体解决方案如下

**先更新数据库，再删缓存**

首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出
* 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
* 命中：应用程序从cache中取数据，取到后返回。
* 更新：先把数据存到数据库中，成功后，再让缓存失效。

另外，知名社交网站facebook也在论文《Scaling Memcache at Facebook》中提出，他们用的也是先更新数据库，再删缓存的策略。

这种情况不存在并发问题么？
不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生
* 缓存刚好失效
* 请求A查询数据库，得一个旧值
* 请求B将新值写入数据库
* 请求B删除缓存
* 请求A将查到的旧值写入缓存

ok，如果发生上述情况，确实是会发生脏数据。

然而，发生这种情况的概率又有多少呢？

发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。
假设，有人非要抬杠，有强迫症，一定要解决怎么办？

如何解决上述并发问题？
首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。
还有其他造成不一致的原因么？
有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。这也是缓存更新策略（2）里留下的最后一个疑问。

如何解决？
提供一个保障的重试机制即可，这里给出两套方案。

方案一：
如下图所示
![](https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_update1.png)
流程如下所示
* （1）更新数据库数据；
* （2）缓存因为种种问题删除失败
* （3）将需要删除的key发送至消息队列
* （4）自己消费消息，获得需要删除的key
* （5）继续重试删除操作，直到成功

然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。

方案二：
![](https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_update2.png)
流程如下图所示：
* （1）更新数据库数据
* （2）数据库会将操作信息写入binlog日志当中
* （3）订阅程序提取出所需要的数据以及key
* （4）另起一段非业务代码，获得该信息
* （5）尝试删除缓存操作，发现删除失败
* （6）将这些信息发送至消息队列
* （7）重新从消息队列中获得该数据，重试操作。

备注说明：上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。


#### 1.8.1.8. 缓存异常处理
<a href="#menu" >目录</a>

##### 1.8.1.8.1. 缓存穿透
缓存击穿表示恶意用户模拟请求很多缓存中不存在的数据，由于缓存中都没有，导致这些请求短时间内直接落在了数据库上，导致数据库异常。这个我们在实际项目就遇到了，有些抢购活动、秒杀活动的接口API被大量的恶意用户刷，导致短时间内数据库宕机了，好在数据库是多主多从的，hold住了。

##### 1.8.1.8.2. 缓存击穿
对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。
缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

##### 1.8.1.8.3. 缓存雪崩
缓存在同一时间内大量键过期（失效），接着来的一大波请求瞬间都落在了数据库中导致连接异常。

##### 1.8.1.8.4. 解决方案

**一、 缓存空数据**
如果数据库查询不到数据，仍将向缓存存入一个空数据。

**二、 使用互斥锁排队**

业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中load数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）就够了。


```java
public String getWithLock( String key, Jedis jedis, String lockKey, String uniqueId, long expireTime )
{
	/* 通过key获取value */
	String value = redisService.get( key );
	if ( StringUtil.isEmpty( value ) )
	{
		/*
		 * 分布式锁，详细可以参考https://blog.csdn.net/fanrenxiang/article/details/79803037
		 * 封装的tryDistributedLock包括setnx和expire两个功能，在低版本的redis中不支持
		 */
		try {
			boolean locked = redisService.tryDistributedLock( jedis, lockKey, uniqueId, expireTime );
			if ( locked )
			{
				value = userService.getById( key );
				redisService.set( key, value );
				redisService.del( lockKey );
				return(value);
			} else {
				/* 其它线程进来了没获取到锁便等待50ms后重试 */
				Thread.sleep( 50 );
				getWithLock( key, jedis, lockKey, uniqueId, expireTime );
			}
		} catch ( Exception e ) {
			log.error( "getWithLock exception=" + e );
			return(value);
		} finally {
			redisService.releaseDistributedLock( jedis, lockKey, uniqueId );
		}
	}
	return(value);
}
```

这样做思路比较清晰，也从一定程度上减轻数据库压力，但是锁机制使得逻辑的复杂度增加，吞吐量也降低了，有点治标不治本。

**三、 布隆过滤器（推荐）**

bloomfilter就类似于一个hash set，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断一个key是否存在于某容器，不存在就直接返回。布隆过滤器的关键就在于hash算法和容器大小，下面先来简单的实现下看看效果，我这里用guava实现的布隆过滤器：

```xml
 <dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version> 23.0 </version>
    </dependency>
 </dependencies >
```
```java
 public class BloomFilterTest {
	 private static final int capacity	= 1000000;
	 private static final int key		= 999998;
	 private static BloomFilter<Integer> bloomFilter = BloomFilter.create( Funnels.integerFunnel(), capacity );
	 static {
		 for ( int i = 0; i < capacity; i++ )
		 {
			 bloomFilter.put( i );
		 }
	 }
	 public static void main( String[] args )
	 {
 /*返回计算机最精确的时间，单位微妙*/
		 long start = System.nanoTime();
		 if ( bloomFilter.mightContain( key ) )
		 {
			 System.out.println( "成功过滤到" + key );
		 }
		 long end = System.nanoTime();
		 System.out.println( "布隆过滤器消耗时间:" + (end - start) );
		 int sum = 0;
		 for ( int i = capacity + 20000; i < capacity + 30000; i++ )
		 {
			 if ( bloomFilter.mightContain( i ) )
			 {
				 sum = sum + 1;
			 }
		 }
		 System.out.println( "错判率为:" + sum );
	 }
 }
```
 
 成功过滤到999998
 布隆过滤器消 耗 时间 : 215518
 错判率 为 : 318
可以看到，100w个数据中只消耗了约0.2毫秒就匹配到了key，速度足够快。然后模拟了1w个不存在于布隆过滤器中的key，匹配错误率为318/10000，也就是说，出错率大概为3%，跟踪下BloomFilter的源码发现默认的容错率就是0.03：

```java
public static < T > BloomFilter<T> create( Funnel<T> funnel, int expectedInsertions)
{
	return(create( funnel, expectedInsertions, 0.03 ) ); /* FYI, for 3%, we always get 5 hash functions */
}
```
我们可调用BloomFilter的这个方法显式的指定误判率：

```java
private static BloomFilter<Integer> bloomFilter = BloomFilter.create(Funnels.integerFunnel(), capacity,0.01);
```

我们断点跟踪下，误判率为0.02和默认的0.03时候的区别:

对比两个出错率可以发现，误判率为0.02时数组大小为8142363，0.03时为7298440，误判率降低了0.01，BloomFilter维护的数组大小也减少了843923，可见BloomFilter默认的误判率0.03是设计者权衡系统性能后得出的值。要注意的是，布隆过滤器不支持删除操作。用在这边解决缓存穿透问题就是：
```java
public String getByKey( String key )
{
	/* 通过key获取value */
	String value = redisService.get( key );
	if ( StringUtil.isEmpty( value ) )
	{
		if ( bloomFilter.mightContain( key ) )
		{
			value = userService.getById( key );
			redisService.set( key, value );
			return(value);
		} else {
			return(null);
		}
	}
	return(value);
}
```

**四、永远不过期**
不过期则不会出现失效问题，可以解决缓存击穿和雪崩问题。

**五、建立备份缓存，设置多级缓存**
缓存A和缓存B，A设置超时时间，B不设值超时时间，先从A读缓存，A没有读B，并且更新A缓存和B缓存;

```java
public String getByKey( String keyA, String keyB )
{
	String value = redisService.get( keyA );
	if ( StringUtil.isEmpty( value ) )
	{
		value = redisService.get( keyB );
		String newValue = getFromDbById();
		redisService.set( keyA, newValue, 31, TimeUnit.DAYS );
		redisService.set( keyB, newValue );
	}
	return(value);
}
```

##### 1.8.1.8.5. 缓存并发问题

这里的并发指的是多个redis的client同时set key引起的并发问题。比较有效的解决方案就是把redis.set操作放在队列中使其串行化，必须的一个一个执行，具体的代码就不上了，当然加锁也是可以的，至于为什么不用redis中的事务，留给各位看官自己思考探究。


### 1.8.2. WEB缓存&客户端缓存
<a href="#menu" >目录</a>

#### 1.8.2.1. 应用APP缓存

#### 1.8.2.2. 浏览器缓存

* Cookie
    * Cookie 是小甜饼的意思。顾名思义，cookie 确实非常小，它的大小限制为4KB左右。它的主要用途有保存登录信息，比如你登录某个网站市场可以看到“记住密码”，这通常就是通过在 Cookie 中存入一段辨别用户身份的数据来实现的。

* localStorage
    * localStorage 是 HTML5 标准中新加入的技术，它并不是什么划时代的新东西。早在 IE 6 时代，就有一个叫 userData 的东西用于本地存储，而当时考虑到浏览器兼容性，更通用的方案是使用 Flash。而如今，localStorage 被大多数浏览器所支持，如果你的网站需要支持 IE6+，那以 userData 作为你的 polyfill 的方案是种不错的选择。

* sessionStorage
    * sessionStorage 与 localStorage 的接口类似，但保存数据的生命周期与 localStorage 不同。做过后端开发的同学应该知道 Session 这个词的意思，直译过来是“会话”。而 sessionStorage 是一个前端的概念，它只是可以将一部分数据在当前会话中保存下来，刷新页面数据依旧存在。但当页面关闭后，sessionStorage 中的数据就会被清空。

|特性|	Cookie|	localStorage|	sessionStorage|
|---|---|---|---|
|数据的生命期|	一般由服务器生成，可设置失效时间。如果在浏览器端生成Cookie，默认是关闭浏览器后失效|	除非被清除，否则永久保存|	仅在当前会话下有效，关闭页面或浏览器后被清除|	仅在当前会话下有效，关闭页面或浏览器后被清除
|存放数据大小|	4K左右|  	一般为5MB|一般为5MB|
|与服务器端通信	|每次都会携带在HTTP头中，如果使用cookie保存过多数据会带来性能问题	|仅在客户端（即浏览器）中保存，不参与和服务器的通信|仅在客户端（即浏览器）中保存，不参与和服务器的通信
|易用性	|需要程序员自己封装，源生的Cookie接口不友好	|源生接口可以接受，亦可再次封装来对Object和Array有更好的支持|源生接口可以接受，亦可再次封装来对Object和Array有更好的支持

* 这三者都是无法跨域的。

**应用场景**
因为考虑到每个 HTTP 请求都会带着 Cookie 的信息，所以 Cookie 当然是能精简就精简啦，比较常用的一个应用场景就是判断用户是否登录。针对登录过的用户，服务器端会在他登录时往 Cookie 中插入一段加密过的唯一辨识单一用户的辨识码，下次只要读取这个值就可以判断当前用户是否登录啦。曾经还使用 Cookie 来保存用户在电商网站的购物车信息，如今有了 localStorage，似乎在这个方面也可以给 Cookie 放个假了~

而另一方面 localStorage 接替了 Cookie 管理购物车的工作，同时也能胜任其他一些工作。比如HTML5游戏通常会产生一些本地数据，localStorage 也是非常适用的。如果遇到一些内容特别多的表单，为了优化用户体验，我们可能要把表单页面拆分成多个子页面，然后按步骤引导用户填写。这时候 sessionStorage 的作用就发挥出来了。

**安全性的考虑**
需要注意的是，不是什么数据都适合放在 Cookie、localStorage 和 sessionStorage 中的。使用它们的时候，需要时刻注意是否有代码存在 XSS 注入的风险。因为只要打开控制台，你就随意修改它们的值，也就是说如果你的网站中有 XSS 的风险，它们就能对你的 localStorage 肆意妄为。所以千万不要用它们存储你系统中的敏感数据。

**操作**
localStorage和sessionStorage都具有相同的操作方法，例如setItem、getItem和removeItem，clear
localStorage和sessionStorage没有过期时间和超时回收策略，因此可以保存数据的时候顺便保存当前时间和超时，读取时再检测是否超时。


#### 1.8.2.3. CDN缓存

* **基本概念**
    * CDN的全称是Content Delivery Network，即内容分发网络。CDN是构建在现有网络基础之上的智能虚拟网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。
* **组成**
    * CDN网络中包含的功能实体包括内容缓存设备、内容交换机、内容路由器、CDN内容管理系统等组成。 
    * 内容缓存为CDN网络节点，位于用户接入点，是面向最终用户的内容提供设备，可缓存静态Web内容和流媒体内容，实现内容的边缘传播和存储，以便用户的就近访问。 
    * 内容交换机处于用户接入集中点，可以均衡单点多个内容缓存设备的负载，并对内容进行缓存负载平衡及访问控制 
    * 内容路由器负责将用户的请求调度到适当的设备上。内容路由通常通过负载均衡系统来实现，动态均衡各个内容缓存站点的载荷分配，为用户的请求选择最佳的访问站点，同时提高网站的可用性。内容路由器可根据多种因素制定路由，包括站点与用户的临近度、内容的可用性、网络负载、设备状况等。负载均衡系统是整个CDN的核心。负载均衡的准确性和效率直接决定了整个CDN的效率和性能。
    * 内容管理系统负责整个CDN的管理，是可选部件，作用是进行内容管理，如内容的注入和发布、内容的分发、内容的审核、内容的服务等。 
* **功能**
    * 节省骨干网带宽，减少带宽需求量； 
    * 提供服务器端加速，解决由于用户访问量大造成的服务器过载问题；
    * 服务商能使用Web Cache技术在本地缓存用户访问过的Web页面和对象，实现相同对象的访问无须占用主干的出口带宽，并提高用户访问因特网页面的相应时间的需求；
    * 能克服网站分布不均的问题，并且能降低网站自身建设和维护成本； 
    * 降低“通信风暴”的影响，提高网络访问的稳定性。 
* **基本原理**
    * CDN的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布到用户访问相对集中的地区或网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求。 
    * CDN的基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。通过在网络各处放置节点服务器所构成的在现有的互联网基础之上的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。 
* **服务模式**
    * 内容分发网络（CDN）是一种新型网络构建方式，它是为能在传统的IP网发布宽带丰富媒体而特别优化的网络覆盖层；而从广义的角度，CDN代表了一种基于质量与秩序的网络服务模式。 
    * 简单地说，内容分发网络（CDN）是一个经策略性部署的整体系统，包括分布式存储、负载均衡、网络请求的重定向和内容管理4个要件，而内容管理和全局的网络流量管理（Traffic Management）是CDN的核心所在。通过用户就近性和服务器负载的判断，CDN确保内容以一种极为高效的方式为用户的请求提供服务。  
    * 总的来说，内容服务基于缓存服务器，也称作代理缓存（Surrogate），它位于网络的边缘，距用户仅有"一跳"（Single Hop）之遥。同时，代理缓存是内容提供商源服务器（通常位于CDN服务提供商的数据中心）的一个透明镜像。这样的架构使得CDN服务提供商能够代表他们客户，即内容供应商，向最终用户提供尽可能好的体验，而这些用户是不能容忍请求响应时间有任何延迟的。  
* **主要特点**
    * 本地Cache加速：提高了企业站点（尤其含有大量图片和静态页面站点）的访问速度，并大大提高以上性质站点的稳定性。  
    * 镜像服务：消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量。 
    * 远程加速：远程访问用户根据DNS负载均衡技术智能自动选择Cache服务器，选择最快的Cache服务器，加快远程访问的速度。 
    * 带宽优化：自动生成服务器的远程Mirror（镜像）cache服务器，远程用户访问时从cache服务器上读取数据，减少远程访问的带宽、分担网络流量、减轻原站点WEB服务器负载等功能。
    * 集群抗攻击：广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的影响，同时保证较好的服务质量 。 
* **关键技术**
    * 内容发布
        * 它借助于建立索引、缓存、流分裂、组播（Multicast）等技术，将内容发布或投递到距离用户最近的远程服务点（POP）处。
        * 内容分发包含从内容源到CDN边缘的Cache的过程。从实现上，有两种主流的内容分发技术：PUSH和PULL。 
        * PUSH是一种主动分发的技术。通常，PUSH由内容管理系统发起，将内容从源或者中心媒体资源库分发到各边缘的 Cache节点。分发的协议可以采用 Http/ftp等。通过PUSH分发的内容一般是比较热点的内容，这些内容通过PUSH方式预分发（ Preload）到边缘Cache，可以实现有针对的内容提供。对于PUSH分发需要考虑的主要问题是分发策略，即在什么时候分发什么内容。一般来说，内容分发可以由CP（内容提供商）或者CDN内容管理员人工确定，也可以通过智能的方式决定，即所谓的智能分发，它根据用户访问的统计信息，以及预定义的内容分发的规则，确定内容分发的过程PULL是一种被动的分发技术，PULL分发通常由用户请求驱动。当用户请求的内容在本地的边缘 Cache上不存在（未命中）时， Cache启动PUL方法从内容源或者其他CDN节点实时获取内容。在PULL方式下，内容的分发是按需的。 
    * 内容路由
        * 它是整体性的网络负载均衡技术，通过内容路由器中的重定向（DNS）机制，在多个远程POP上均衡用户的请求，以使用户请求得到最近内容源的响应。 
        * CDN负载均衡系统实现CDN的内容路由功能。它的作用是将用户的请求导向整个CDN网络中的最佳节点。最佳节点的选定可以根据多种策略，例如距离最近、节点负载最轻等。负载均衡系统是整个CDN的核心，负载均衡的准确性和效率直接决定了整个CDN的效率和性能。通常负载均衡可以分为两个层次:全局负载均衡（GSLB）和本地负载均衡（SLB）。全局负载均衡主要的目的是在整个网络范围内将用户的请求定向到最近的节点（或者区域）。因此，就近性判断是全局负载均衡的主要功能。本地负载均衡一般局限于一定的区域范围内，其目标是在特定的区域范围内寻找一台最适合的节点提供服务，因此，CDN节点的健康性、负载情况、支持的媒体格式等运行状态是本地负载均衡进行决策的主要依据。 
    * 内容存储
        * 对于CDN系统而言，需要考虑两个方面的内容存储问题。一个是内容源的存储，一个是内容在 Cache节点中的存储。
        * 对于内容源的存储，由于内容的规模比较大（通常可以达到几个甚至几十个TB），而且内容的吞吐量较大，因此，通常采用海量存储架构，如NAS和SON。对于在 Cache节点中的存储，是 Cache设计的一个关键问题。需要考虑的因素包括功能和性能两个方面:功能上包括对各种内容格式的支持，对部分缓存的支持;在性能上包括支持的容量、多文件吞吐率、可靠性、稳定性。
        * 其中，多种内容格式的支持要求存储系统根据不同文件格式的读写特点进行优化，以提高文件内容读写的效率。特别是对针对流媒体文件的读写。部分缓存能力指流媒体内容可以以不完整的方式存储和读取。部分缓存的需求来自用户访问行为的随机性，因为许多用户并不会完整地收看整个流媒体节目。事实上，许多用户访问单个流媒体节目的时间不超过10分钟。因此，部分缓存能力能够大大提高存储空间的利用率，并有效提高用户请求的响应时间。但是部分缓存可能导致内容的碎片问题，需要进行良好的设计和控制。 
        * Cache存储的另一个重要因素是存储的可靠性，目前，多数存储系统都采用了独立磁盘冗余阵列（RAID）技术进行可靠存储。但是不同设备使用的RAID方式各有不同。 
    * 内容管理
        * 它通过内部和外部监控系统，获取网络部件的状况信息，测量内容发布的端到端性能（如包丢失、延时、平均带宽、启动时间、帧速率等），保证网络处于最佳的运行状态。  
        * 内容管理在广义上涵盖了从内容的发布、注入、分发、调整、传递等一系列过程。在这里，内容管理重点强调内容进人 Cache点后的内容管理，称其为本地内容管理。本地内容管理主要针对一个ODN节点（有多个 CDN Cache设备和一个SLB设备构成）进行。本地内容管理的主要目标是提高内容服务的效率，提高本地节点的存储利用率。通过本地内容管理，可以在CDN节点实现基于内容感知的调度，通过内容感知的调度，可以避免将用户重定向到没有该内容的 Cache设备上，从而提高负载均衡的效率。通过本地内容管理还可以有效实现在ODN节点内容的存储共享，提高存储空间的利用率

**浏览器访问网站流程**
* 没有CDN的时候
    * 用户向浏览器提交要访问的域名
    * 浏览器对域名进行解析，得到域名对应的IP地址
    * 浏览器向所得到的IP地址发送请求
    * 浏览器根据返回的数据进行显示
* 存在CDN的时候
    * 用户向浏览器提交要访问的域名
    * 浏览器对域名进行解析
    * 由于CDN对域名解析过程进行了调整，所以得到的是该域名对应的CNAME记录
    * 对CNAME再次进行解析，得到实际的IP地址。
        * 使用全局负载均衡DNS解析，获取到最近的访问IP地址
        * 需要根据地理位置和所在的ISP来确定返回结果
        * 让身处不同地域，连接不同接入商的用户得到最适合自己访问的CDN地址，才能做到最近访问，从而提升速度
    * 得到实际的IP地址，向服务器发出请求
    * 如果不存在，则CDN请求源站，获取内容，然后再返回结果
* 关键技术
    * 全局调度
    * 缓存技术
    * 内容分发
    * 带宽优化
* CDN意义
    * 把资源放到离用户近的地方，从而提高访问速度
    * 可以让用户上传的文件传到CDN，CDN再传到源站，从而提高上传速度
    
#### 1.8.2.4. NGINX缓存


### 1.8.3. 多级缓存
<a href="#menu" >目录</a>

#### 1.8.3.1. 多级缓存介绍

多级缓存就是在整个系统架构的不同系统层级进行数据缓存。以提升访问效率。

![](https://images2018.cnblogs.com/blog/531691/201802/531691-20180223174306584-2092797691.png)

* 前端做缓存，比如不常更新的数据进行缓存，比如图片，用户数据等，同时借助CDN实现静态文件的缓存和快速访问
* 接入Nginx将请求负载均衡到应用Nginx，此处常用的负载均衡算法是轮询或者一致性哈希，轮询可以使服务器的请求更加均衡，而一致性哈希可以提升应用Nginx的缓存命中率，相对于轮询，一致性哈希会存在单机热点问题，一种解决办法是热点直接推送到接入层Nginx，一种办法是设置一个阀值，当超过阀值，改为轮询算法。
* 接着应用Nginx读取本地缓存（本地缓存可以使用Lua Shared Dict、Nginx Proxy Cache（磁盘/内存）、Local Redis实现），如果本地缓存命中则直接返回，使用应用Nginx本地缓存可以提升整体的吞吐量，降低后端的压力，尤其应对热点问题非常有效。
* 如果Nginx本地缓存没命中，则会读取相应的分布式缓存（如Redis缓存，另外可以考虑使用主从架构来提升性能和吞吐量），如果分布式缓存命中则直接返回相应数据（并回写到Nginx本地缓存）。
* 如果分布式缓存也没有命中，则会回源到Tomcat集群，在回源到Tomcat集群时也可以使用轮询和一致性哈希作为负载均衡算法。
* 在Tomcat应用中，首先读取本地堆缓存，如果有则直接返回（并会写到主Redis集群），为什么要加一层本地堆缓存将在缓存崩溃与快速修复部分细聊。
* 作为可选部分，如果步骤4没有命中可以再尝试一次读主Redis集群操作。目的是防止当从有问题时的流量冲击。
* 如果所有缓存都没有命中只能查询DB或相关服务获取相关数据并返回。
* 步骤7返回的数据异步写到主Redis集群，此处可能多个Tomcat实例同时写主Redis集群，可能造成数据错乱，如何解决该问题将在更新缓存与原子性部分细聊。

应用整体分了三部分缓存：应用Nginx本地缓存、分布式缓存、Tomcat堆缓存，每一层缓存都用来解决相关的问题，如应用Nginx本地缓存用来解决热点缓存问题，分布式缓存用来减少访问回源率、Tomcat堆缓存用于防止相关缓存失效/崩溃之后的冲击。

#### 1.8.3.2. 如何缓存数据

接下来部将从缓存过期、维度化缓存、增量缓存、大Value缓存、热点缓存几个方面来详细介绍如何缓存数据。

##### 1.8.3.2.1. 过期与不过期
对于缓存的数据我们可以考虑不过期缓存和带过期时间缓存，什么场景应该选择哪种模式需要根据业务和数据量等因素来决定。

* 不过期缓存场景一般思路如图所示：
![不过期缓存方案](https://images2018.cnblogs.com/blog/531691/201802/531691-20180223174453437-1543922855.png) 

使用Cache-Aside模式，首先写数据库，如果成功，则写缓存。这种场景下存在事务成功、缓存写失败但无法回滚事务的情况。另外，不要把写缓存放在事务中，尤其写分布式缓存，因为网络抖动可能导致写缓存响应时间很慢，引起数据库事务阻塞。如果对缓存数据一致性要求不是那么高，数据量也不是很大，则可以考虑定期全量同步缓存。 

也有提到如下思路：先删缓存，然后执行数据库事务；不过这种操作对于如商品这种查询非常频繁的业务不适用，因为在你删缓存的同时，已经有另一个系统来读缓存了，此时事务还没有提交。当然对于如用户维度的业务是可以考虑的。

不过为了更好地解决以上多个事务的问题，可以考虑使用订阅数据库日志的架构，如使用canal订阅mysql的binlog实现缓存同步。

对于长尾访问的数据、大多数数据访问频率都很高的场景、缓存空间足够都可以考虑不过期缓存，比如用户、分类、商品、价格、订单等，当缓存满了可以考虑LRU机制驱逐老的缓存数据。

* 过期缓存机制
即采用懒加载，一般用于缓存别的系统的数据（无法订阅变更消息、或者成本很高）、缓存空间有限、低频热点缓存等场景；常见步骤是：首先读取缓存如果不命中则查询数据，然后异步写入缓存并过期缓存，设置过期时间，下次读取将命中缓存。热点数据经常使用即在应用系统上缓存比较短的时间。这种缓存可能存在一段时间的数据不一致情况，需要根据场景来决定如何设置过期时间。如库存数据可以在前端应用上缓存几秒钟，短时间的不一致时可以忍受的。

##### 1.8.3.2.2. 维度化缓存与增量缓存
对于电商系统，一个商品可能拆成如基础属性、图片列表、上下架、规格参数、商品介绍等；如果商品变更了要把这些数据都更新一遍那么整个更新成本很高：接口调用量和带宽；因此最好将数据进行维度化并增量更新（只更新变的部分）。尤其如上下架这种只是一个状态变更，但是每天频繁调用的，维度化后能减少服务很大的压力。
按照不同维度接收MQ进行更新。

##### 1.8.3.2.3. 大Value 缓存
要警惕缓存中的大Value，尤其是使用Redis时。遇到这种情况时可以考虑使用多线程实现的缓存，如Memcached，来缓存大Value；或者对Value进行压缩；或者将Value拆分为多个小Value，客户端再进行查询、聚合。

 

##### 1.8.3.2.4. 热点缓存
对于那些访问非常频繁的热点缓存，如果每次都去远程缓存系统中获取，可能会因为访问量太大导致远程缓存系统请求过多、负载过高或者带宽过高等问题，最终可能导致缓存响应慢，使客户端请求超时。一种解决方案是通过挂更多的从缓存，客户端通过负载均衡机制读取从缓存系统数据。不过也可以在客户端所在的应用/ 代理层本地存储一份，从而避免访问远程缓存，即使像库存这种数据，在有些应用系统中也可以进行几秒钟的本地缓存，从而降低远程系统的压力。

#### 1.8.3.3. 更新缓存和原子性

如果多个应用同时操作一份数据，很可能导致缓存数据变成脏数据．
解决：
* 更新数据时使用更新时间戳或者版本进行对比，如果是redis,可以利用其单线程机制进行原子化更新
* 使用如cannal订阅数据库的binlog
* 将更新请求按照相应的规则分散到多个队列，然后每个队列进行单线程更新，更新时拉取最新的数据保存
* 使用分布式锁，在更新之前获取相关的锁



### 1.8.4. Feed
<a href="#menu" >目录</a>


#### 1.8.4.1. 概念

是一种呈现内容给用户并持续更新的方式，用户可以选择订阅多个资源，网站提供feed 网址 ，用户将feed网址登记到阅读器里，在阅读器里形成的聚合页就是feed流。

feed可以理解为一条信息，比如一条朋友圈。feed流就是多条朋友圈组成的信息流。

比如微信微博的刷新页面的数据流。


我们在讲如何设计Feed流系统之前，先来看一下Feed流中的一些概念：
* Feed：Feed流中的每一条状态或者消息都是Feed，比如朋友圈中的一个状态就是一个Feed，微博中的一条微博就是一个Feed。
* Feed流：持续更新并呈现给用户内容的信息流。每个人的朋友圈，微博关注页等等都是一个Feed流。
* Timeline：Timeline其实是一种Feed流的类型，微博，朋友圈都是Timeline类型的Feed流，但是由于Timeline类型出现最早，使用最广泛，最为人熟知，有时候也用Timeline来表示Feed流。
* 关注页Timeline：展示其他人Feed消息的页面，比如朋友圈，微博的首页等。
* 个人页Timeline：展示自己发送过的Feed消息的页面，比如微信中的相册，微博的个人页等。




* feed流产品的两个核心问题：
    * 应该展示给用户什么内容
    * 这些内容应该如何排序
        * 时间排序法
            * 这种排序法最简单粗暴，就是按照内容的发布时间先后进行排序
            * 时间排序有一个致命的缺点：内容呈现效率最为低下。运用这种排序，需要内容的提供方非常克制，同时也需要用户对这些内容足够关注
            * 朋友圈刚好满足这两点：内容是微信好友发布，注定不会大量更新；微信好友大多是熟人关系，能够引起用户足够的关注。
        * 重力排序法
            * 由重力和拉力，共同决定feed流中内容的排序
                * 所谓的”重力“，是持续让内容在feed流往下掉的力，即时间
                * 而拉力，即让内容排序往前的力
                    * 这个拉力，由应用根据自己关注的参数来决定，比如豆瓣小组的回应、贴吧的回复。
                    * 在知乎上被赞得多的帖子，或者贴吧里得到更多回复的帖子会更靠前
            * 应该展示给用户什么内容：一群用户表现出喜好的内容都推给某个用户
            * 这些内容该怎么排序：按照时间衰减因素和内容受欢迎程度综合排序。
        * 机器算法排序法
            * 通过一定的算法算出用户的喜好而向他进行相关的推荐
            * 智能推荐排序法，将用户关心的内容进行了更加细化的测量，从而将用户本人更感兴趣的内容推送出来，而不是重力排序法中，那些热门的内容
            * 以Facebook早期的EdgeRank算法为例,影响feed流排序，主要有三个因素：
                * 亲密度（Affinity Score）：考虑该信息的来源者和你之间交流是否频繁密切。例如你女朋友发的一条状态肯定比某个不太熟的同学发的要重要。
                * 生产成本（Edge Weight）：成本越高权重越大。例如好友发布了9张图片的成本比起发了9个字成本高，前者就会被优先推荐；又例如发布的成本远高于点赞，所以原创内容的优先级高于因为好友点赞而被你获知的消息。
                * 新鲜程度（Time Decay）：越近发生的事越容易被推荐，一般都是用一个指数衰减函数来量化动态的新旧程度。
        * 如果选择时间排序法，那么就需要考虑用户关注的内容是否足够有吸引力。
        * 如果选择重力排序法，那么就需要考虑该如何选择参数保证最后的展示效果。
        * 如果选择智能推荐排序法，那么就需要考虑是否有足够的技术实力和产品自制力。

#### 1.8.4.2. 特征
Feed流系统有一些非常典型的特点，比如：
* 多账号内容流：Feed流系统中肯定会存在成千上万的账号，账号之间可以关注，取关，加好友和拉黑等操作。只要满足这一条，那么就可以当做Feed流系统来设计。
* 非稳定的账号关系：由于存在关注，取关等操作，所以系统中的用户之间的关系就会一直在变化，是一种非稳定的状态。
* 读写比例100:1：读写严重不平衡，读多写少，一般读写比例在10：1，甚至100：1以上。
* 消息必达性要求高：比如发送了一条朋友圈后，结果部分朋友看到了，部分朋友没看到，如果偏偏女朋友没看到，那么可能会产生很严重的感情矛盾，后果很严重。

#### 1.8.4.3. 分类
Feed流的分类有很多种，但最常见的分类有两种：
* Timeline：按发布的时间顺序排序，先发布的先看到，后发布的排列在最顶端，类似于微信朋友圈，微博等。这也是一种最常见的形式。产品如果选择Timeline类型，那么就是认为Feed流中的Feed不多，但是每个Feed都很重要，都需要用户看到。
* Rank：按某个非时间的因子排序，一般是按照用户的喜好度排序，用户最喜欢的排在最前面，次喜欢的排在后面。这种一般假定用户可能看到的Feed非常多，而用户花费在这里的时间有限，那么就为用户选择出用户最想看的Top N结果，场景的应用场景有图片分享、新闻推荐类、商品推荐等。

上面两种是最典型，也是最常见的分类方式，另外的话，也有其他的分类标准，在其他的分类标准中的话，会多出两种类型：
* Aggregate：聚合类型，比如好几个朋友都看了同一场电影，这个就可以聚合为一条Feed：A，B，C看了电影《你的名字》，这种聚合功能比较适合在客户端做。一般的Aggregate类型是Timeline类型 + 客户端聚合。
* Notice：通知类型，这种其实已经是功能类型了，通知类型一般用于APP中的各种通知，私信等常见。这种也是Timeline类型，或者是Aggregate类型。


#### 1.8.4.4. 实现
上面介绍了Feed流系统的概念，特征以及分类，接下来开始进入关键部分：如何实现一个千万级Feed流系统。由于系统中的所有用户不可能全部在线，且不可能同时刷新和发布Feed，那么一个能支撑千万量级Feed流的系统，其实在产品上可以支撑上亿的用户。

如果要设计一个Feed流系统，最关键的两个核心，一个是存储，一个是推送。

**存储**
我们先来看存储，Feed流系统中需要存储的内容分为两部分，一个是账号关系（比如关注列表），一种是Feed消息内容。不管是存储哪一种，都有几个问题需要考虑：
* 如何能支持100TB，甚至PB级数据量？
* 数据量大了后成本就很关键，成本如何能更便宜？
* 如何保证账号关系和Feed不丢失？


**推送**
推送系统需要的功能有两个，一个是发布Feed，一个是读取Feed流。对于提送系统，仍然有一些问题需要在选型之前考虑：
* 如何才能提供千万的TPS和QPS？
* 如何保证读写延迟在10ms，甚至2ms以下？
* 如何保证Feed的必达性？

再解答这些问题之前，我们先来大概了解下阿里云的表格存储TableStore。

#### 1.8.4.5. TableStore

表格存储(TableStore)是阿里云自主研发的专业级分布式NoSQL数据库，是基于共享存储的高性能、低成本、易扩展、全托管的半结构化数据存储平台，
支撑互联网和物联网数据的高效计算与分析。

目前不管是阿里巴巴集团内部，还是外部公有云用户，都有成千上万的系统在使用。覆盖了重吞吐的离线应用，以及重稳定性，性能敏感的在线应用。目前使用的系统中，有些系统每秒写入行数超过3500万行，每秒流量超过5GB，单表总行数超过10万亿行，单表数据量超过10PB。

表格存储的具体的特性可以看下面这张图片

![](https://yqfile.alicdn.com/65fb03c38abcf8c4865f3b745d20c132620e49c2.png)

这里就不详细介绍表格存储(TableStore)的功能和特性了，有兴趣的话可以到官网页面和云栖博客了解，地址如下：

表格存储的官网地址：https://www.aliyun.com/product/ots/
表格存储云栖博客：https://yq.aliyun.com/teams/4/type_blog-cid_22

#### 1.8.4.6. 存储系统选择
我们接下来解决之前提出来的问题。
Feed流系统中需要存储的系统有两类，一类是账号关系（比如关注列表），一类是Feed消息。

**存储账号关系**
我们先来看账号关系（比如关注列表）的存储，对于账号关系，它有一些特点：
* 是一系列的变长链表，长度可达亿级别。
* 这样就会导致数据量比较大，但是关系极其简单。
* 还有一点是性能敏感，直接影响关注，取关的响应速度。
* 最适合存账号关系（关注列表）的系统应该是分布式NoSQL数据库，原因是数据量极大，关系简单不需要复杂的join，性能要求高。
* 对内设计实现简单，对外用户体验好。

除了上面这些特点外，还有一个特点：
* 有序性：有序性并不要求具有排序功能，只需要能按照主键排序就行，只要能按照主键排序，那么关注列表和粉丝列表的顺序就是固定的，可预期的。

**使用开源HBase存储账号关系**
能满足有序性的分布式NoSQL数据库中，开源HBase就是一个，所以很多企业会选择开源HBase来存储账号关系，或者是关注列表。

这样虽然满足了上述四个特征，可以把系统搭建起来，但是会有一些麻烦的问题：
* 需要自己运维，调查问题，Fix bug，会带来较大的复杂度和成本开支。
* GC会导致比较大的毛刺，影响用户体验

**使用表格存储(TableStore)存储账号关系**
除此之外，阿里云的表格存储也属于有序性的分布式NoSQL数据库，之前有不少很有名的系统选择使用表格存储，在下面一些地方给系统带来了收益：
* 单表支持10万亿行+，10PB+的数据量，再快的数据增长速度都不用担心。
* 数据按主键列排序，保证有序性和可预期性。
* 单key读写延迟在毫秒级别，保证关注，取关的响应时间。
* 是全托管的分布式NoSQL数据库服务，无需任何运维。
* 全部采用C++ 实现，彻底无GC问题，也就不会由于GC而导致较大的毛刺。
* 使用表格存储(TableStore)来存储账号关系会是一个比较好的选择。

#### 1.8.4.7. 存储Feed消息

**Feed消息有一个最大的特点：**
* 数据量大，而且在Feed流系统里面很多时候都会选择写扩散（推模式）模式，这时候数据量会再膨胀几个数量级，所以这里的数据量很容易达到100TB，甚至PB级别。

**除此之外，还有一些其他特点：**
* 数据格式简单
* 数据不能丢失，可靠性要求高
* 自增主键功能，保证个人发的Feed的消息ID在个人发件箱中都是严格递增的，这样读取时只需要一个范围读取即可。由于个人发布的Feed并发度很低，这里用时间戳也能满足基本需求，但是当应用层队列堵塞，网络延迟变大或时间回退时，用时间戳还是无法保证严格递增。这里最好是有自增功能。
* 成本越低越好

**潜在的存储系统**
根据上述这些特征，最佳的系统应该是具有主键自增功能的分布式NoSQL数据库，但是在开源系统里面没有，所以常用的做法有两种：
* 关系型数据库 + 分库分表
* 关系型数据库 + 分布式NoSQL数据库：其中 关系型数据库提供主键自增功能。

**使用关系型数据库存储Feed消息**
目前业界有很多用户选择了关系系数据库+ 分库分表，包括了一些非常著名的Feed流产品，虽然这个架构可以运行起来，但是存在一些问题。
* 分库分表带来了运维复杂性。
* 分库分表带来了逻辑层和数据层的极大耦合性。
* 关系型数据库，比如开源MySQL数据库的主键自增功能性能差。不管是用MyISAM，还是InnoDB引擎，要保证自增ID严格递增，必须使用表锁，这个粒度非常大，会严重限制并发度，影响性能。
* 有些用户觉得关系型数据库的可靠性高一些，但是关系型数据库的可靠性一般也就最多6个9，这个可靠性和分布式数据库完全不在一个层级，要低4到5个级别。

**使用TableStore存储账号关系**
基于上述原因，一些技术公司开始考虑使用表格存储(TableStore)，表格存储是一个具有自增主键功能的分布式NoSQL数据库，这样就只需要使用一种系统，除此之外还有以下的考虑：
* 单表可达10PB，10万亿行。
* 10个9的SLA保障Feed内容不丢失。
* 天然分布式数据库，无需分库分表
* 两种实例类型：高性能实例采用全SSD存储媒介，提供极佳的读写性能。混合存储实例采用SSD+SATA存储媒介，提供极低的存储成本。
* 主键自增功能性能极佳，其他所有系统在做自增功能的时候都需要加锁，但是表格存储的主键自增功能在写入自增列行的时候，完全不需要锁，既不需要表锁，也不需要行锁。

从上面看，使用TableStore的话，不管是在功能，性能，扩展性还是成本方面都要更加适合一些。

看完推送系统选择后，我们再来看看推送方案的选择。

#### 1.8.4.8. 推送方案
我们先来回顾下之前说的Feed流系统最大的特点：
* 读写严重不平衡，读多写少，一般读写比例都在10；1，甚至100：1之上。

除此之外，还有一个方面会被推送方案影响：
* 发布， 刷新Feed时的延时本质上由推送方案决定，其他的任何操作都只能是优化，质量量变，无法质变。

**推模式和拉模式对比**
在推送方案里面的，有两种方案，分别是：
* 拉方案：也称为读扩散。
* 推方案：也成为写扩散。

对于拉方案和推方案，他们在很多方面完全相反，在看对比之前有一点要强调下：

* 对Feed流产品的用户而言，刷新Feed流（读取）时候的延迟敏感度要远远大于发布（写入）的时候。

|拉模式(读扩散)	|推模式(写扩散)||
|---|---|---|
|发布|	个人页Timeline（发件箱）|	粉丝的关注页（收件箱）|
|阅读	|所有关注者的个人页Timeline	|自己的关注页Timeline
|网络最大开销|	用户刷新时|	发布Feed时
|读写放大|	放大读：读写比例到1万:1	|放大写减少读：读写比例到50:50
|个性化	|不支持	|支持
|定向广告|	不支持|	支持

**推模式的一个副作用**
在上面的对比中可以明显看出来，推模式要远远比拉模式更好一些，但是也有一个副作用：
* 数据会极大膨胀。

针对这个缺点，可以从两个方面考虑：
* 目前的存储价格很低很低了，就以表格存储为例，容量型实例存储10TB的数据量，在现在（2017年10月）每年费用是1万六千元，以后价格会随着硬件技术升级，软件性能优化等继续降低。还有数据量越大价格越便宜。
* 想省点钱，那继续可以优化：
    * 对大V采用拉模式，普通用户使用推模式，这种模式有个缺点，后面会有分析。
    * 对活跃粉丝采用推模式，非活跃粉丝采用拉模式（这种方式可以较好的避免大流量对平台的冲击）

**适用场景**
通过上述两个方案的对比后，总结下各个方案的适用场景：
* 拉模式：
    * 很多Feed流产品的第一版会采用这种方案，但很快就会抛弃。
    * 另外，拉模式 + 图计算 就会是另一番天地，但是这个时候重心就是图计算了。
* 推模式：
    * Feed流系统中最常用、有效的模式；
    * 用户关系数比较均匀，或者有上限，比如朋友圈；
    * 偏推荐类，同一个Feed对不同用户价值不同，需要为不同用户计算分数，比如pinterest。
* 推拉结合
    * 大部分用户的账号关系都是几百个，但是有个别用户是1000万以上，比如微博。


#### 1.8.4.9. 推送系统
如果要实现一个千万量级的Feed流产品，那么推送系统需要具备一些特点：
* 具备千万TPS/QPS的能力。
* 读写链路延迟敏感，读写直接会影响用户发布，刷新Feed流时的延迟，尤其是极其敏感的刷新时的延迟。
* Feed消息的必达性要求很高。
* 主键自增功能，仍然是保证用户收件箱中的Feed ID是严格递增的，保证可以通过Scan(上次读取的最大ID --->MAX)读取到最新未读消息。
* 最好能为用户存储Timeline中所有的Feed。

从上述特点来看，需要的推送系统最好是一个性能极佳，又可靠的有自增功能的NoSQL系统，所以，业内一般如果选择开源系统的话，会在选择了关系型数据库作为存储系统的基础上，选择开源Redis，这样就能覆盖上述的几个特征，也能保证Feed流系统正常运行起来，但是也会带来一些其他问题：
* 纯内存系统，内存价格极高，整体成本就比较高了。
* 属于单机系统，为了支持千万TPS和保证消息必达性，需要使用cluster和replica模式，结果就是不仅带来了运维的复杂性，而且带来了成本的机器增加，成本再次上升。
* 成本上升了以后，就有架构师开始考虑是否可以节省一些成本，要节省成本只能是减少开源Redis里面存储的数据量，一般有两种做法，这两种做法都能减少存入Redis中的数据量：
    * 只在开源Redis中存储Feed ID，不存储Feed内容。整体数据量会大量减少，但是在读取的时候需要先读Feed ID，然后在到存储系统里面去读取Feed内容，网络开销增长了一倍，而且是串行的，对用户的刷新延迟有较大影响。
    * 只对普通用户或者活跃用户使用推模式，对大V和非活跃用户直接使用拉模式。


上述两个方案虽然可以节省成本，但是是以牺牲用户体验为代价的，最终需要在成本和用户体验之间权衡。

**使用TableStore作为推送系统**
除了使用开源系统外，还可以使用阿里云的表格存储（TableStore），有不少用户选择TableStore作为推送系统的原因无非下面几点：
* 天然分布式，单表可支持千万级TPS/QPS。
* LSM存储引擎极大优化写，高性能实例极大优化读。
* 写入成功即保证落盘成功，数据可靠性提供10个9的SLA保障。
* 磁盘性数据库，费用比内存性的要低几个量级。
* 单表可存储十万亿行以上的数据，价格又低，轻松保存用户Feed流中的所有Feed数据。

上面说了使用开源Redis和阿里云TableStore的异同，如果使用开源可以用Redis，如果选择阿里云自研NoSQL数据库，可以使用TableStore。

#### 1.8.4.10. 使用TableStore的架构图
![](https://yqfile.alicdn.com/ac5465cc451372d20cafeca586a51b7560a46c1c.png)

**存储**
我们先来看中间黑色框中的部分，这部分是使用TableStore的数据，从左往右分别是：
* 个人页Timeline：这个是每个用户的发件箱，也就是自己的个人页页面。
* 关注页Timeline：这个是每个用户的收件箱，也就是自己的关注页页面，内容都是自己关注人发布的消息。
* 关注列表：保存账号关系，比如朋友圈中的好友关系；微博中的关注列表等。
* 虚拟关注列表：这个主要用来个性化和广告。

**发布Feed流程**
当你发布一条Feed消息的时候，流程是这样的：
* Feed消息先进入一个队列服务。
* 先从关注列表中读取到自己的粉丝列表，以及判断自己是否是大V。
* 将自己的Feed消息写入个人页Timeline（发件箱）。如果是大V，写入流程到此就结束了。
* 如果是普通用户，还需要将自己的Feed消息写给自己的粉丝，如果有100个粉丝，那么就要写给100个用户，包括Feed内容和Feed ID。
* 第三步和第四步可以合并在一起，使用BatchWriteRow接口一次性将多行数据写入TableStore。
* 发布Feed的流程到此结束。

**读取Feed流流程**
当刷新自己的Feed流的时候，流程是这样的：
* 先去读取自己关注的大V列表
* 去读取自己的收件箱，只需要一个GetRange读取一个范围即可，范围起始位置是上次读取到的最新Feed的ID，结束位置可以使当前时间，也*可以是MAX，建议是MAX值。由于之前使用了主键自增功能，所以这里可以使用GetRange读取。
* 如果有关注的大V，则再次并发读取每一个大V的发件箱，如果关注了10个大V，那么则需要10次访问。
* 合并2和3步的结果，然后按时间排序，返回给用户。

至此，使用推拉结合方式的发布，读取Feed流的流程都结束了。

**更简单的推模式**
如果只是用推模式了，则会更加简单：

* 发布Feed：
    * 不用区分是否大V，所有用户的流程都一样，都是三步。
* 读取Feed流：
    * 不需要第一步，也不需要第三步，只需要第二步即可，将之前的2 + N(N是关注的大V个数) 次网络开销减少为 1 次网络开销。读取延时大幅降级。

**个性化和定向广告**
个性化和定向广告是两种很强烈的产品需求。个性化可以服务好用户，增大产品竞争力和用户粘性，而定向广告可以为产品增加盈利渠道，而且还可以不招来用户反感，那么这两种方式如何实现呢？ 在Feeds流里面这两种功能的实现方式差不多，我们以定向广告为例来说明：
* 通过用户特征分析对用户分类，比如其中有一类是新生类：今年刚上大学的新生。（具体的用户特征分析可以依靠TableStore + MaxCompute，这里就不说了）。
* 创建一个广告账号：新生广告
* 让这些具有新生特征的用户虚拟关注新生广告账号。用户看不到这一层关注关系。
* 从七月份开始就可以通过新生广告账号发送广告了。
* 最终，每个用户可能会有多个特征，那么就可能虚拟关注多个广告账号。
上面是定向广告的一种比较简单的实现方式，其他方式就不再赘述了。

**收益**
上面我们详细说了使用TableStore作为存储和推送系统的架构，接下来我们看看新架构能给我们带来多大收益。
* 只使用1种系统，架构、实现简单。不再需要访问多个系统，架构，开发，测试，运维都能节省大力人力时间。
* TableStore 主键自增列功能性能极优。由于架构的不同，不仅不需要表锁，行锁也不需要，所以性能要远远好于关系型数据库。
* 可以保存所有的Feed。一是系统可以支持存储所有Feed，二是价格便宜，存的起。
* 无须将Feed ID和内容分开存储。价格便宜，也就不需要再分开存储ID和内容了。
* 全托管服务，无运维操作，更无需分库分表。
* 磁盘型(SSD、Hybrid)数据库，成本低。
* 可靠性10个9，数据更可靠，更不易丢失。
* 大V和普通用户的切分阈值更高，读取大V的次数更少，整体延时更低。

**一个设计缺陷**
如果使用大V/普通用户的切分方式，大V使用拉模式，普通用户使用推模式，那么这种架构就会存在一种很大的风险。
比如某个大V突然发了一个很有话题性的Feed，那么就有可能导致整个Feed产品中的所有用户都没法读取新内容了，原因是这样的：
* 大V发送Feed消息。
* 大V，使用拉模式。
* 大V的活跃粉丝（用户群A）开始通过拉模式（架构图中读取的步骤3，简称读3）读取大V的新Feed。
* Feed内容太有话题性了，快速传播。
* 未登录的大V粉丝（用户群B）开始登陆产品，登陆进去后自动刷新，再次通过读3步骤读取大V的Feed内容。
* 非粉丝（用户群C）去大V的个人页Timeline里面去围观，再次需要读取大V个人的Timeline，同读3.

结果就是，平时正常流量只有用户群A，结果现在却是用户群A + 用户群B+ 用户群C，流量增加了好几倍，甚至几十倍，导致读3路径的服务模块被打到server busy或者机器资源被打满，导致读取大V的读3路径无法返回请求，如果Feed产品中的用户都有关注大V，那么基本上所有用户都会卡死在读取大V的读3路径上，然后就没法刷新了。

所以这里设计的时候就需要重点关心下面两点：
* 单个模块的不可用，不应该阻止整个关键的读Feed流路径，如果大V的无法读取，但是普通用户的要能返回，等服务恢复后，再补齐大V的内容即可。
* 当模块无法承受这么大流量的时候，模块不应该完全不可服务，而应该能继续提供最大的服务能力，超过的拒绝掉。

那么如何优化呢？
* 不使用大V/普通用户的优化方式，使用活跃用户/非活跃用户的优化方式。这样的话，就能把用户群A和部分用户群B分流到其他更分散的多个路径上去。而且，就算读3路径不可用，仍然对活跃用户无任何影响。
* 完全使用推模式就可以彻底解决这个问题，但是会带来存储量增大，大V微博发送总时间增大，从发给第一个粉丝到发给最后一个粉丝可能要几分钟时间（一亿粉丝，100万行每秒，需要100秒），还要为最大并发预留好资源，如果使用表格存储，因为是云服务，则不需要考虑预留最大额度资源的问题。

#### 1.8.4.11. 实践
接下来我们来实现一个消息广场的功能。很多App中都有动态或消息广场的功能，在消息广场中一般有两个Tab，一个是关注人，一个是广场，我们这里重点来看关注人。

* 要实现的功能如下：
    * 用户之间可以相互关注
    * 用户可以发布新消息
    * 用户可以查看自己发布的消息列表
    * 用户可以查看自己关注的人的消息
* 采取前面的方案：
    * 使用TableStore作为存储和推送系统
    * 采用Timeline的显示方式，希望用户可以认真看每条Feed
    * 采用推模式

**角色**
接着，我们看看角色和每个角色需要的功能：
* 发送者
    * 发送状态：add_activity()
* 接收者
    * 关注：follow()
    * 读取Feed流：get_activity()

* Feed消息中至少需要包括下面内容：
    * 消息：
    * 发送人：actor
    * 类型：verb，比如图片，视频，文本
    * 文本文字：message

**架构图**

![](https://yqfile.alicdn.com/778795c6c911b155561ad551dfda9d4afee7f842.png)

* 发布新消息
    * 接口：add_activity()
    * 实现：
        * get_range接口调用关注列表，返回粉丝列表。
        * batch_write_row接口将feed内容和ID批量写入个人页表（发件箱）和所有粉丝的关注页表（收件箱），如果量太大，可以多次写入。或者调用异步batch_write_row接口，目前C++ SDK和JAVA SDK提供异步接口。
* 关注
    * 接口：follow()
    * 实现：
        * put_row接口直接写入一行数据(关注人，粉丝)到关注列表和粉丝列表（粉丝，关注人）即可。
* 获取Feed流消息
    * 接口：get_activity()
    * 实现：
        * 从客户端获取上次读取到的最新消息的ID：last_id
        * 使用get_range接口读取最新的消息，起始位置是last_id，结束位置是MAX。
        * 如果是读取个人页的内容，访问个人页表即可。如果是读取关注页的内容，访问关注页表即可。
**计划**
上面展示了如何使用表格存储TableStore的API来实现。这个虽然只用到几个接口，但是仍然需要学习表格存储的API和特性，还是有点费时间。

为了更加易用性，我们接下来会提供Feeds流完整解决方案，提供一个LIB，接口直接是add_activity()，follow()和get_activity()类似的接口，使用上会更加简单和快捷。

**扩展**
前面讲述的都是Timeline类型的Feed流类型，但是还有一种Feed流类型比较常见，那就是新闻推荐，图片分享网站常用的Rank类型。
我们再来回顾下Rank类型擅长的领域：
* 潜在Feed内容非常多，用户无法全部看完，也不需要全部看完，那么需要为用户选出她最想看的内容，典型的就是图片分享网站，新闻推荐网站等。

我们先来看一种架构图：

![](https://yqfile.alicdn.com/996e90e2e725e16ef78e9803898eb5277dcb9f8a.png)
* 这种Rank方式比较轻量级，适用于推拉结合的场景。
* 写流程基本一样
* 读流程里面会先读取所有的Feed内容，这个和Timeline也一样，Timeline里面的话，这里会直接返回给用户，但是Rank类型需要在一个排序模块里面，按照某个属性值排序，然后将所有结果存入一个timeline cache中，并返回分数最高的N个结果，下次读取的时候再返回[N+1, 2N]的结果。

再来看另外一种：

![](https://yqfile.alicdn.com/f7b38cc6c04fdb16200bdcb559b55190d060df19.png)

* 这种比较重量级，适用于纯推模式。
* 写流程也和Timeline一样。
* 每个用户有两个收件箱：
    * 一个是关注页Timeline，保存原始的Feed内容，用户无法直接查看这个收件箱。
    * 一个是rank timeline，保存为用户精选的Feed内容，用户直接查看这个收件箱。
* 写流程结束后还有一个数据处理的流程。个性化排序系统从原始Feed收件箱中获取到新的Feed 内容，按照用户的特征，Feed的特征计算出一个分数，每个Feed在不同用户的Timeline中可能分数不一样的，计算完成后再排序然后写入最终的rank timeline。
* 这种方式可以真正为每个用户做到“千人千面”。

上述两种方式是实现Rank的比较简单，常用的方式。

#### 1.8.4.12. 最后
从上面的内容来看，表格存储(TableStore)在存储方面可以支持10PB级，推送方面可以支撑每秒千万的TPS/QPS，在Feed流系统中可以发挥很大的价值。

目前，已经有不少著名公司在使用表格存储(TableStore)来构建他们自己的Feed流系统，最终为系统，产品，公司都带来了不少收益


## 1.9. 系统稳定性
<a href="#menu" >目录</a>

### 1.9.1. 在线日志分析

#### 1.9.1.1. 日志分析常用命令

**查看文件内容 cat**
```bash
cat xxx.log
```

**分页显示文件 more/less**
* Enter显示文件下一行
* 空格键下一页
* F显示下一屏
* B显示上一屏

* less 支持内容查找和高亮显示，进入之后输入命令 /xxx

**显示文件尾 tail**
```bash
tail -n 10 -f xxx.log
```
-n 显示的行数
-f 动态显示

**显示文件头 head**
```bash
head -n 10  xxx.log
```
**内容排序 sort**

```bash
sort -n xx.log
```

**字符串查找 grep**

```bash
grep  123  xx.log

# 正则表达式A开头B结尾
grep 'A.*B' xx.log
```

**文件查找 find**
```bash
fina / -name xxx
fina / -name "*.txt"

```
**查找可执行文件的位置 whereis**
```bash
whereis nginx
```

**sed**

**awk**



### 1.9.2. 集群监控

<a href="#menu" >目录</a>

#### 1.9.2.1. 监控指标



### 1.9.3. 流量控制

### 1.9.4. 性能优化

### 1.9.5. Java故障排查

